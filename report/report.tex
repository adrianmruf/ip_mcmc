% Created 2020-04-24 Fr 17:06
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{physics}
\newcommand{\C}{{\mathcal{C}}}
\newcommand{\I}{{\mathcal{I}}}
\newcommand{\G}[1]{{\mathcal{G} \left( #1 \right)}}
\newcommand{\N}[2]{\mathcal{N}\left(#1,#2\right)}
\author{David Ochsner}
\date{\today}
\title{Markov Chain Monte Carlo for Inverse Problems}
\hypersetup{
 pdfauthor={David Ochsner},
 pdftitle={Markov Chain Monte Carlo for Inverse Problems},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.1.9)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{Theory}
\label{sec:orgac9a778}
\subsection{Papers}
\label{sec:org4f29f07}
\subsubsection{Stuart et al: Inverse Problems: A Bayesian Perspective \cite{stuart_inverse_2010}}
\label{sec:orgc1ba1d3}
Theoretical Background
\subsubsection{Cotter et al: MCMC for functions \cite{cotter_mcmc_2013}}
\label{sec:org3c89c32}
Implementation, MCMC in infinite dimensions
\subsubsection{Schneider et al: Earth System Modeling 2.0  \cite{schneider_earth_2017}}
\label{sec:org77ea5f6}
Example for MCMC on ODE
\subsection{Small results}
\label{sec:org429d1f2}
\subsubsection{Bayes' Formula \& Radon-Nikodym Derivative}
\label{sec:org76f6132}
Bayes' Formula is stated using the Radon-Nikodym Derivative in both \cite{cotter_mcmc_2013} and \cite{stuart_inverse_2010}:
$$\dv{\mu}{\mu_0} \propto \text{L}(u),$$
where \(\text{L}(u)\) is the likelihood.

Write the measures as \(\dd \mu = \rho(u)\dd u\) and \(\dd \mu_0 = \rho_0(u)\dd u\) with respect
to the standard Lesbesgue measure. Then we have
$$
    \int f(u) \rho(u) \dd u =
    \int f(u) \dd \mu(u) =
    \int f(u) \dv{\mu(u)}{\mu_0(u)} \dd \mu_0 =
    \int f(u) \dv{\mu(u)}{\mu_0(u)} \rho_0(u) \dd u,
    $$
provided that \(\dd \mu\), \(\dd \mu_0\) and \(f\) are nice enough (which they are since we're working
with Gaussians). This holds for all test functions \(f\), so it must hold pointwise:
$$ \dv{\mu(u)}{\mu_0(u)} = \frac{\rho(u)}{\rho_o(u)}.$$

Using this we recover the more familiar formulation of Bayes' formula:
$$\frac{\rho(u)}{\rho_o(u)} \propto \text{L}(u).$$

\subsubsection{Acceptance Probability for Metropolis-Hastings}
\label{sec:org1c2fb5c}
A Markov process with transition probabilities \(t(y|x)\) has a stationary distribution \(\pi(x)\).
\begin{itemize}
\item The \uline{existence} of \(\pi(x)\) follows from \emph{detailed balance}:
$$\pi(x)t(y|x) = \pi(y)t(x|y).$$
Detailed balance is sufficient but not necessary for the existence of a stationary distribution.
\item \uline{Uniqueness} of \(\pi(x)\) follows from the Ergodicity of the Markov process. For a Markov
processto be Ergodic it has to:
\begin{itemize}
\item not return to the same state in a fixed interval
\item reach every state from every other state in finite time
\end{itemize}
\end{itemize}

The Metropolis-Hastings algorithm constructs transition probabilities \(t(y|x)\) such that the
two conditions above are satisfied and that \(\pi(x) = P(x)\), where \(P(x)\) is the distribution
we want to sample from.

Rewrite detailed balance as
$$\frac{t(y|x)}{t(x|y)} = \frac{P(y)}{P(x)}.$$
Split up the transition probability into proposal \(g(y|x)\) and acceptance \(a(y,x)\). Then detailed
balance requires
$$\frac{a(y,x)}{a(x,y)} = \frac{P(y)g(x|y)}{P(x)g(y|x)}.$$
Choose
$$a(y,x) = \min\left\{1, \frac{P(y)g(x|y)}{P(x)g(y|x)}\right\}$$
to ensure that detailed balance is always satisfied. Choose \(g(y|x)\) such that ergodicity
is fulfilled.

If the proposal is symmetric (\(g(y|x) = g(x|y)\)), then the acceptance takes the simpler form
$$a(y,x) = \min\left\{1, \frac{P(y)}{P(x)}\right\}.$$

Since the target distribution \(P(x)\) only appears as a ratio, normalizing factors can be ignored.
\subsubsection{Acceptance Probabilities for different MCMC Proposers}
\label{sec:orgbfbcbe1}
Start from Bayes' formula and rewrite the likelyhood \(\text{L}(u)\) as \(\exp(-\Phi(u))\) for
a positive scalar function \(\Phi\) called the potential:
$$\frac{\rho(u)}{\rho_o(u)} \propto \exp(\Phi(u)).$$
Assuming our prior to be a Gaussian (\(\mu_0 \sim \N{0}{\C}\)).

(IS WRITING \(\rho_0(u) \propto \exp(-\frac{1}{2} u^TC^{-1}u)\) ASSUMING FINITE DIMENSIONS? WHAT
ABOUT \(\rho_0(u) \propto \exp(-\frac{1}{2} \norm{C^{-1/2}u}^2)\)? I assume the former is not,
for \(C\) to be a proper covariance operator it should be invertible. But taking the square root
is probably not always well defined for infinite dimensions (so the latter one is problematic))

Then $$\rho(u) \propto \exp\left( -\Phi(u) + \frac{1}{2} \norm{C^{-1/2}u}^2 \right),$$
since \(u^T C^{-1} u = (C^{-1/2} u)^T(C^{-1/2} u) = \langle C^{-1/2}u, C^{-1/2}u \rangle = \norm{C^{-1/2} u}^2\),
where in the first equality we used \(C\) being symmetric.

This is formula (1.2) in \cite{cotter_mcmc_2013} and is used in the acceptance probability for
the standard random walk (see also \hyperref[sec:org1c2fb5c]{Acceptance Probability for Metropolis-Hastings})

\(\C^{-1/2}u\) makes problems in infinite dimensions.

Todo: Why exactly is the second term (from the prior) cancelled when doing pCN?
\subsubsection{Different formulations of multivariate Gaussians}
\label{sec:org9b541e4}
THIS WHOLE SECTION ASSUMES FINITE DIMENSIONS

Is an RV \(\xi \sim \N{0}{C}\) distributed the same as \(C^{1/2}\xi_0\), with \(\xi_0 \sim \N(0, \I)\)?

Is \(C^{1/2}\exp(\frac{1}{2} x^Tx) = \exp(\frac{1}{2} x^T C^{-1} x)\) ?

From wikipedia: Affine transformation \(Y = c + BX\) for \(X \sim \N{\mu}{\Sigma}\) is also a Gaussian
\(Y \sim \N{c + B\mu}{B\Sigma B^T}\). In our case \(X \sim \N{0}{I}\), so \(Y \sim \N{0}{C^{1/2}\I {C^{1/2}}^{T}} = \N{0}{C}\),
since the covariance matrix is positive definite, which means it's square root is also positive definite
and thus symmetric.

\section{Implementation}
\label{sec:orgd82f85b}
\subsection{Framework/Package Structure}
\label{sec:org1973776}
The framework is designed to support an easy use case:
\begin{minted}[]{python}
proposer = StandardRWProposer(beta=0.25, dims=1)
accepter = AnalyticAccepter(my_distribution)
rng = np.random.default_rng(42)
sampler = MCMCSampler(rw_proposer, accepter, rng)

samples = sampler.run(x_0=0, n_samples=1000)
\end{minted}

There is only one source of randomness, shared among all classes and supplied by the user.
This facilitates reproducability.

Tests are done with \texttt{pytest}.
\subsubsection{Distributions}
\label{sec:org8f9bfa5}
A class for implementing probability distributions.
\begin{minted}[]{python}
class DistributionBase(ABC):
    @abstractmethod
    def sample(self, rng):
        """Return a point sampled from this distribution"""
        ...
\end{minted}

The most important realisation is the \texttt{GaussianDistribution}, used
in the proposers.

\begin{minted}[]{python}
class GaussianDistribution(DistributionBase):
    def __init__(self, mean=0, covariance=1):
        ...

    def sample(self, rng):
        ...

    def apply_covariance(self, x):
        ...

    def apply_sqrt_covariance(self, x):
        ...

    def apply_precision(self, x):
        ...

    def apply_sqrt_precision(self, x):
        ...
\end{minted}

The design of this class is based on the implementation in \href{http://muq.mit.edu/master-muq2-docs/CrankNicolson\_8py\_source.html}{muq2}. The \texttt{precision} / \texttt{sqrt\_precision}
is implemented through a Cholesky decomposition, computed in the constructor. This makes
applying them pretty fast (\(\mathcal{O}(n^2)\)).

At the moment the there is one class for both scalar and multivariate Gaussians. This
introduces some overhead as it has to work with both \texttt{float} and \texttt{np.array}. Maybe two
seperate classes would be better.
\subsubsection{Proposers}
\label{sec:org58a3d5a}

Propose a new state \(v\) based on the current one \(u\).

\begin{minted}[]{python}
class ProposerBase(ABC):
    @abstractmethod
    def __call__(self, u, rng):
        ...
\end{minted}

\begin{enumerate}
\item StandardRWProposer
\label{sec:org623ce20}

Propose a new state as
$$v = u + \sqrt{2\delta} \xi,$$
with either \(\xi \sim \N{0}{\I}\) or \(\xi \sim \N{0}{\C}\) (see section 4.2 in \cite{cotter_mcmc_2013}).

This leads to a well-defined algorithm in finite dimensions.
This is not the case when working on functions (as described in section 6.3 in \cite{cotter_mcmc_2013})

\item pCNProposer
\label{sec:org7442233}

Propose a new state as
$$v = \sqrt{1-\beta^2} u + \beta \xi,$$
with \(\xi \sim \N{0}{\C}\) and \(\beta = \frac{8\delta}{(2+\delta)^2} \in [0,1]\)
(see formula (4.8) in \cite{cotter_mcmc_2013}).

This approach leads to an improved algorithm (quicker decorrelation in finite dimensions,
nicer properties for infinite dimensions)(see sections 6.2 + 6.3 in \cite{cotter_mcmc_2013}).

The wikipedia-article on the Cholesky-factorization mentions the use-case of obtaining a
correlated sample from an uncorrelated one by the Cholesky-factor. This is not implemented here.
\end{enumerate}
\subsubsection{Accepters}
\label{sec:org840936f}

Given a current state \(u\) and a proposed state \(v\), decide if the new state is accepted or rejected.

For sampling from a distribution \(P(x)\), the acceptance probability for a symmetric proposal is
\(a = \text{min}\{1, \frac{P(v)}{P(u)}\}\)
(see \ref{sec:org1c2fb5c})

\begin{minted}[]{python}
class ProbabilisticAccepter(AccepterBase):
    def __call__(self, u, v, rng):
        """Return True if v is accepted"""
        a = self.accept_probability(u, v)
        return a > rng.random()

    @abstractmethod
    def accept_probability(self, u, v):
        ...
\end{minted}

\begin{enumerate}
\item AnalyticAccepter
\label{sec:orgfad4738}

Used when there is an analytic expression of the desired distribution.

\begin{minted}[]{python}
class AnalyticAccepter(ProbabilisticAccepter):
    def accept_probability(self, u, v):
        return self.rho(v) / self.rho(u)
\end{minted}

\item StandardRWAccepter
\label{sec:orgef2747a}

Based on formula (1.2) in \cite{cotter_mcmc_2013}:
$$a = \text{min}\{1, \exp(I(u) - I(v))\},$$ with
$$I(u) = \Phi(u) + \frac{1}{2}\norm{\C^{-1/2}u}^2$$.

See also \ref{sec:orgbfbcbe1}.

\item pCNAccepter
\label{sec:orga79b011}

Works together with the \hyperref[sec:org7442233]{pCNProposer} to achieve the simpler expression for the acceptance
$$a = \text{min}\{1, \exp(\Phi(u) - \Phi(v))\}.$$

\item CountedAccepter
\label{sec:orgedec74e}

Stores and forwards calls to an "actual" accepter. Counts calls and accepts and is used for
calculating the acceptance ratio.
\end{enumerate}

\subsubsection{Sampler}
\label{sec:orgc42d820}

The structure of the sampler is quite simple, since it can rely heavily on the functionality
provided by the Proposers and Accepters.

\begin{minted}[]{python}
class MCMCSampler:
    def __init__(self, proposal, acceptance, rng):
        ...

    def run(self, u_0, n_samples, burn_in=1000, sample_interval=200):
        ...

    def _step(self, u, rng):
        ...
\end{minted}

\subsection{Results}
\label{sec:org5afe8ff}
\subsubsection{Analytic sampling from a bimodal Gaussian}
\label{sec:orgc2e4e38}
\subsubsection{Bayesian inverse problem for \(\G{u} = \langle g,u \rangle\)}
\label{sec:orge157133}
\subsubsection{Bayesian inverse problem for \(\G{u} = g (u + \beta u^3)\)}
\label{sec:orgca667cf}
\subsubsection{Geophysics example}
\label{sec:orgff276a7}


\bibliographystyle{plain}
\bibliography{../papers/inverse_problems}
\end{document}
