% Created 2020-05-19 Di 12:09
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{physics}
\usepackage{dsfont}
\newcommand{\C}{{\mathcal{C}}}
\newcommand{\I}{{\mathcal{I}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\G}[1]{{\mathcal{G} \left( #1 \right)}}
\newcommand{\N}[2]{\mathcal{N}\left(#1,#2\right)}
\author{David Ochsner}
\date{\today}
\title{Markov Chain Monte Carlo for Inverse Problems}
\hypersetup{
 pdfauthor={David Ochsner},
 pdftitle={Markov Chain Monte Carlo for Inverse Problems},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.1.9)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{Theory}
\label{sec:org010c7b4}
\subsection{Papers}
\label{sec:orgb03e070}
\subsubsection{Stuart et al: Inverse Problems: A Bayesian Perspective \cite{stuart_inverse_2010}}
\label{sec:org62d3a00}
Theoretical Background \footnote{this is a test footnote}

\paragraph{Notation}
\label{sec:orge3e969b}

Central equation:
$$y = \G{u} + \eta$$
with:
\begin{itemize}
\item \(y \in \R^q\): data
\item \(u \in \R^n\): IC ("input to mathematical model")
\item \(\G{\cdot} :\R^n \to \R^q\): observation operator
\item \(\eta\): mean zero RV, observational noise (a.s. \(\eta \sim \N{0}{\C}\))
\end{itemize}
\subsubsection{Cotter et al: MCMC for functions \cite{cotter_mcmc_2013}}
\label{sec:org1e8d743}
Implementation, MCMC in infinite dimensions
\subsubsection{Schneider et al: Earth System Modeling 2.0  \cite{schneider_earth_2017}}
\label{sec:orgce03e0f}
Example for MCMC on ODE
\subsection{Small results}
\label{sec:org8801cc9}
\subsubsection{Gaussian in infinite dimensions}
\label{sec:org1aeaf62}
This section is quite a mess, maybe you could suggest a not-too-technical introduction
to infinite dimensional Gaussian measures?

Wiki: Definition of Gaussian measure uses Lesbesgue measure.
However, the Lesbesgue-Measure is not defined in an infinite-dimensional space (\href{https://en.wikipedia.org/wiki/Infinite-dimensional\_Lebesgue\_measure}{wiki}).

Can still define a measure to be Gaussian if we demand all push-forward measures via a
linear functional onto \(\R\) to be a Gaussian. (What about the star (E\(^{\text{*}}\), L\(_{\text{*}}\))
in the wiki-article? Are they dual-spaces?) (What would be an example of that? An example
for a linear functional on an inf-dims space given on wikipedia is integration.
What do we integrate? How does this lead to a Gaussian?)

How does this fit with the description in \cite{cotter_mcmc_2013}? -> Karhunenen-Lo√©ve

What would be an example of a covariance operator in infinite dimensions?
The Laplace-Operator operates on functions, the eigenfunctions would be \(sin\), \(cos\) (I think?
This might not actually be so easy, see \href{https://en.wikipedia.org/wiki/Dirichlet\_eigenvalue}{Dirichlet Eigenvalues}). Are the eigenvalues
square-summable?

Anyway, when a inf-dim Gaussian is given as a KL-Expansion, an  example of a linear functional
given as \(f(u) = \langle \phi_i, u \rangle\) for \(\phi_i\) an eigenfunction of \(\C\), then I can see
the push-forward definition of inf-dim Gaussians satisfied. ( \(\C\) spd, so \(\phi_i\) s are
orthogonal, so we just end up with one of the KH-"components" which is given to be \(\N{0}{1})\).

The problem is not actually in \(\exp(-1/2x^T\C^{-1}x)\). What about \(\exp(-1/2\norm{\C^{-1/2}x})\)?

What about the terminology in \cite{cotter_mcmc_2013}? Absolutely continuous w.r.t a measure for
example?

How is the square root of an operator defined? For matrices, there seems to be a freedom in
choosing whether \(A = BB\) or \(A = BB^T\) for \(B = A^{1/2}\). The latter definition seems to
be more useful when working with Cholesky factorizations (cf. \url{https://math.stackexchange.com/questions/2767873/why-is-the-square-root-of-cholesky-decomposition-equal-to-the-lower-triangular-m}),
but for example in the wiki-article about the matrix (operator) square root (\url{https://en.wikipedia.org/wiki/Square\_root\_of\_a\_matrix}):
"The Cholesky factorization provides another particular example of square root, which should not be confused with the unique non-negative square root."

\subsubsection{Bayes' Formula \& Radon-Nikodym Derivative}
\label{sec:org8fd9bf6}
Bayes' Formula is stated using the Radon-Nikodym Derivative in both \cite{cotter_mcmc_2013} and \cite{stuart_inverse_2010}:
$$\dv{\mu}{\mu_0} \propto \text{L}(u),$$
where \(\text{L}(u)\) is the likelihood.

Write the measures as \(\dd \mu = \rho(u)\dd u\) and \(\dd \mu_0 = \rho_0(u)\dd u\) with respect
to the standard Lesbesgue measure. Then we have
$$
    \int f(u) \rho(u) \dd u =
    \int f(u) \dd \mu(u) =
    \int f(u) \dv{\mu(u)}{\mu_0(u)} \dd \mu_0 =
    \int f(u) \dv{\mu(u)}{\mu_0(u)} \rho_0(u) \dd u,
    $$
provided that \(\dd \mu\), \(\dd \mu_0\) and \(f\) are nice enough (which they are since we're working
with Gaussians). This holds for all test functions \(f\), so it must hold pointwise:
$$ \dv{\mu(u)}{\mu_0(u)} = \frac{\rho(u)}{\rho_o(u)}.$$

Using this we recover the more familiar formulation of Bayes' formula:
$$\frac{\rho(u)}{\rho_o(u)} \propto \text{L}(u).$$

\subsubsection{Acceptance Probability for Metropolis-Hastings}
\label{sec:org2349635}
A Markov process with transition probabilities \(t(y|x)\) has a stationary distribution \(\pi(x)\).
\begin{itemize}
\item The \uline{existence} of \(\pi(x)\) follows from \emph{detailed balance}:
$$\pi(x)t(y|x) = \pi(y)t(x|y).$$
Detailed balance is sufficient but not necessary for the existence of a stationary distribution.
\item \uline{Uniqueness} of \(\pi(x)\) follows from the Ergodicity of the Markov process. For a Markov
processto be Ergodic it has to:
\begin{itemize}
\item not return to the same state in a fixed interval
\item reach every state from every other state in finite time
\end{itemize}
\end{itemize}

The Metropolis-Hastings algorithm constructs transition probabilities \(t(y|x)\) such that the
two conditions above are satisfied and that \(\pi(x) = P(x)\), where \(P(x)\) is the distribution
we want to sample from.

Rewrite detailed balance as
$$\frac{t(y|x)}{t(x|y)} = \frac{P(y)}{P(x)}.$$
Split up the transition probability into proposal \(g(y|x)\) and acceptance \(a(y,x)\). Then detailed
balance requires
$$\frac{a(y,x)}{a(x,y)} = \frac{P(y)g(x|y)}{P(x)g(y|x)}.$$
Choose
$$a(y,x) = \min\left\{1, \frac{P(y)g(x|y)}{P(x)g(y|x)}\right\}$$
to ensure that detailed balance is always satisfied. Choose \(g(y|x)\) such that ergodicity
is fulfilled.

If the proposal is symmetric (\(g(y|x) = g(x|y)\)), then the acceptance takes the simpler form
\begin{equation}
\label{eqn:acceptance_simple}
a(y,x) = \min\left\{1, \frac{P(y)}{P(x)}\right\}.
\end{equation}

Since the target distribution \(P(x)\) only appears as a ratio, normalizing factors can be ignored.

\subsubsection{Potential for Bayes'-MCMC when sampling from analytic distributions}
\label{sec:orgf8b70c5}
How can we use formulations of Metropolis-Hastings-MCMC algorithms designed to sample from
posteriors when want to sample from probability distribution with an easy analytical expression?

Algorithms for sampling from a posterior sample from
$$\rho(u) \propto \rho_0(u) \exp(-\Phi(u)),$$
where \(\rho_0\) is the prior and \(\exp(-\Phi(u))\) is the likelihood. Normally, we have an
efficient way to compute the likelihood.

When we have an efficient way to compute the posterior \(\rho\) and we want to sample from it,
the potential to do that is:
$$\Phi(u) = \ln(\rho_0(u)) - \ln(\rho(u)),$$
where an additive constant from the normalization was omitted since only potential differences
are relevant.

When working with a Gaussian prior \(\N{0}{\C}\), the potential takes the form
$$\Phi(u) = -\ln{\rho(u)} - \frac{1}{2} \norm{\C^{-1/2}u}^2.$$

When inserting this into the acceptance probability for the standard random walk MCMC given
in formula (1.2) in \cite{cotter_mcmc_2013}, the two Gaussian-expressions cancel, as do the
logarithm and the exponentiation, leaving the simple acceptance described in \ref{eqn:acceptance_simple}.

This cancellation does not happen when using the pCN-Acceptance probablity. This could
explain the poorer performance of pCN when directly sampling a probablity distribution.

\subsubsection{Acceptance Probabilities for different MCMC Proposers}
\label{sec:org4748f6c}
Start from Bayes' formula and rewrite the likelyhood \(\text{L}(u)\) as \(\exp(-\Phi(u))\) for
a positive scalar function \(\Phi\) called the potential:
$$\frac{\rho(u)}{\rho_o(u)} \propto \exp(\Phi(u)).$$
Assuming our prior to be a Gaussian (\(\mu_0 \sim \N{0}{\C}\)).

Then $$\rho(u) \propto \exp\left( -\Phi(u) + \frac{1}{2} \norm{C^{-1/2}u}^2 \right),$$
since \(u^T C^{-1} u = (C^{-1/2} u)^T(C^{-1/2} u) = \langle C^{-1/2}u, C^{-1/2}u \rangle = \norm{C^{-1/2} u}^2\),
where in the first equality we used \(C\) being symmetric.

This is formula (1.2) in \cite{cotter_mcmc_2013} and is used in the acceptance probability for
the standard random walk (see also \hyperref[sec:org2349635]{Acceptance Probability for Metropolis-Hastings})

\(\C^{-1/2}u\) makes problems in infinite dimensions.

Todo: Why exactly is the second term (from the prior) cancelled when doing pCN?
\subsubsection{Different formulations of multivariate Gaussians}
\label{sec:org18e9247}
Is an RV \(\xi \sim \N{0}{C}\) distributed the same as \(C^{1/2}\xi_0\), with \(\xi_0 \sim \N{0}{\I}\)?

From wikipedia: Affine transformation \(Y = c + BX\) for \(X \sim \N{\mu}{\Sigma}\) is also a Gaussian
\(Y \sim \N{c + B\mu}{B\Sigma B^T}\). In our case \(X \sim \N{0}{\I}\), so \(Y \sim \N{0}{C^{1/2}\I {C^{1/2}}^{T}} = \N{0}{C}\),
since the covariance matrix is positive definite, which means it's square root is also positive definite
and thus symmetric.

On second thought, it also follows straight from the definition:
$$
      \mathbf{X} \sim \N{\mu}{\Sigma}
      \Leftrightarrow
      \exists \mu \in \R^k, A \in \R^{k \cross l}
        \text{ s.t. }
        \mathbf{X} = \mu + A\mathbf{Z}
        \text{ with } \mathbf{Z}_n \sim \N{0}{1} \text{ i.i.d}
    $$
where \(\Sigma = AA^T\).

\section{Framework/Package Structure}
\label{sec:org86a74cd}
The framework is designed to support an easy use case:
\begin{minted}[]{python}
proposer = StandardRWProposer(beta=0.25, dims=1)
accepter = AnalyticAccepter(my_distribution)
rng = np.random.default_rng(42)
sampler = MCMCSampler(rw_proposer, accepter, rng)

samples = sampler.run(x_0=0, n_samples=1000)
\end{minted}

There is only one source of randomness, shared among all classes and supplied by the user.
This facilitates reproducability.

Tests are done with \texttt{pytest}.
\subsection{Distributions}
\label{sec:org6ec810e}
A class for implementing probability distributions.
\begin{minted}[]{python}
class DistributionBase(ABC):
    @abstractmethod
    def sample(self, rng):
        """Return a point sampled from this distribution"""
        ...
\end{minted}

The most important realisation is the \texttt{GaussianDistribution}, used
in the proposers.

\begin{minted}[]{python}
class GaussianDistribution(DistributionBase):
    def __init__(self, mean=0, covariance=1):
        ...

    def sample(self, rng):
        ...

    def apply_covariance(self, x):
        ...

    def apply_sqrt_covariance(self, x):
        ...

    def apply_precision(self, x):
        ...

    def apply_sqrt_precision(self, x):
        ...
\end{minted}

The design of this class is based on the implementation in \href{http://muq.mit.edu/master-muq2-docs/CrankNicolson\_8py\_source.html}{muq2}. The \texttt{precision} / \texttt{sqrt\_precision}
is implemented through a Cholesky decomposition, computed in the constructor. This makes
applying them pretty fast (\(\mathcal{O}(n^2)\)).

At the moment the there is one class for both scalar and multivariate Gaussians. This
introduces some overhead as it has to work with both \texttt{float} and \texttt{np.array}. Maybe two
seperate classes would be better.

Also, maybe there is a need to implement a Gaussian using the Karhunen-Lo√©ve-Expansion?
\subsection{Potentials}
\label{sec:orgb32bfa9}
A class for implementing the potential resulting from rewriting the likelihood as
$$\text{L}(u) = \exp(- \Phi(u)).$$

\begin{minted}[]{python}
class PotentialBase(ABC):
"""
    Potential used to express the likelihood;
    d mu(u; y) / d mu_0(u) \propto L(u; y)
    Write L(u; y) as exp(-potential(u; y))
    """
    @abstractmethod
    def __call__(self, u):
        ...

    @abstractmethod
    def exp_minus_potential(self, u):
        ...
\end{minted}

The two functions return \(\Phi(u)\) and \(\exp(-\Phi(u))\) respectively. Depending on the
concrete potential, one or the other is easier to compute.

Potentials are used in the accepters to decide the relative weight of different configurations.
There, the \texttt{PotentialBase.exp\_minus\_potential} is used.
\subsubsection{AnalyticPotential}
\label{sec:org185a9fe}

This potential is used when sampling from an analytically computable probability distribution,
i.e. a known posterior. In this case
$$\exp(-\Phi(u)) = \frac{\rho(u)}{\rho_0(u)},$$
see \url{theory.org}
\subsubsection{EvolutionPotential}
\label{sec:orgd64aeb3}

This potential results when sampling from the model-equation
$$y = \G{u} + \eta,$$
with \(\eta \sim \rho\). The resulting potential can be computed as
$$\exp(-\Phi(u)) = \rho(y - \G{u}).$$

\subsection{Proposers}
\label{sec:org57c9a7a}

Propose a new state \(v\) based on the current one \(u\).

\begin{minted}[]{python}
class ProposerBase(ABC):
    @abstractmethod
    def __call__(self, u, rng):
        ...
\end{minted}

\subsubsection{StandardRWProposer}
\label{sec:org5b55ad1}

Propose a new state as
$$v = u + \sqrt{2\delta} \xi,$$
with either \(\xi \sim \N{0}{\I}\) or \(\xi \sim \N{0}{\C}\) (see section 4.2 in \cite{cotter_mcmc_2013}).

This leads to a well-defined algorithm in finite dimensions.
This is not the case when working on functions (as described in section 6.3 in \cite{cotter_mcmc_2013})

\subsubsection{pCNProposer}
\label{sec:org1253113}

Propose a new state as
$$v = \sqrt{1-\beta^2} u + \beta \xi,$$
with \(\xi \sim \N{0}{\C}\) and \(\beta = \frac{8\delta}{(2+\delta)^2} \in [0,1]\)
(see formula (4.8) in \cite{cotter_mcmc_2013}).

This approach leads to an improved algorithm (quicker decorrelation in finite dimensions,
nicer properties for infinite dimensions)(see sections 6.2 + 6.3 in \cite{cotter_mcmc_2013}).

The wikipedia-article on the Cholesky-factorization mentions the use-case of obtaining a
correlated sample from an uncorrelated one by the Cholesky-factor. This is not implemented here.
\subsection{Accepters}
\label{sec:orgd229901}

Given a current state \(u\) and a proposed state \(v\), decide if the new state is accepted or rejected.

For sampling from a distribution \(P(x)\), the acceptance probability for a symmetric proposal is
\(a = \text{min}\{1, \frac{P(v)}{P(u)}\}\)
(see \url{theory.org})

\begin{minted}[]{python}
class ProbabilisticAccepter(AccepterBase):
    def __call__(self, u, v, rng):
        """Return True if v is accepted"""
        a = self.accept_probability(u, v)
        return a > rng.random()

    @abstractmethod
    def accept_probability(self, u, v):
        ...
\end{minted}

\subsubsection{AnalyticAccepter}
\label{sec:org05d74ce}

Used when there is an analytic expression of the desired distribution.

\begin{minted}[]{python}
class AnalyticAccepter(ProbabilisticAccepter):
    def accept_probability(self, u, v):
        return self.rho(v) / self.rho(u)
\end{minted}

\subsubsection{StandardRWAccepter}
\label{sec:orga92bd8d}

Based on formula (1.2) in \cite{cotter_mcmc_2013}:
$$a = \text{min}\{1, \exp(I(u) - I(v))\},$$ with
$$I(u) = \Phi(u) + \frac{1}{2}\norm{\C^{-1/2}u}^2$$.

See also \url{theory.org}.

\subsubsection{pCNAccepter}
\label{sec:orge50bf67}

Works together with the \hyperref[sec:org1253113]{pCNProposer} to achieve the simpler expression for the acceptance
$$a = \text{min}\{1, \exp(\Phi(u) - \Phi(v))\}.$$

\subsubsection{CountedAccepter}
\label{sec:org938869b}

Stores and forwards calls to an "actual" accepter. Counts calls and accepts and is used for
calculating the acceptance ratio.

\subsection{Sampler}
\label{sec:orgb1900cc}

The structure of the sampler is quite simple, since it can rely heavily on the functionality
provided by the Proposers and Accepters.

\begin{minted}[]{python}
class MCMCSampler:
    def __init__(self, proposal, acceptance, rng):
        ...

    def run(self, u_0, n_samples, burn_in=1000, sample_interval=200):
        ...

    def _step(self, u, rng):
        ...
\end{minted}

\section{Results}
\label{sec:org1d4dd2d}
\subsection{Analytic sampling from a bimodal Gaussian}
\label{sec:orgfd7badd}
\subsubsection{Setup}
\label{sec:org708f212}

Attempting to recreate the "Computational Illustration" from \cite{cotter_mcmc_2013}. They use,
among other algorithms, pCN to sample from a 1-D bimodal Gaussian
$$\rho \propto (\N{3}{1} + \N{-3}{1}) \mathds{1}_{[-10,10]}.$$
Since the density estimation framework for a known distribution is not quite clear to me from
the paper, I don't expect to perfectly replicate their results.

They use a formulation of the prior based on the Karhunen-Lo√©ve Expansion that doesn't make
sense to me in the 1-D setting (how do I sum infinite eigenfunctions of a scalar?).

The potential for density estimation described in section is also not clear to me (maybe for
a similar reason? What is \(u\) in the density estimate case?).

I ended up using a normal \(\N{0}{1}\) as a prior and the potential described \hyperref[sec:orgf8b70c5]{before}, and
compared the following samplers:
\begin{itemize}
\item (1) \href{code.org}{\texttt{StandardRWProposer}} (\(\delta=0.25\)) + \href{code.org}{\texttt{AnalyticAccepter}}
\item (2) \href{code.org}{\texttt{StandardRWProposer}} (\(\delta=0.25\)) + \href{code.org}{\texttt{StandardRWAccepter}}
\item (3) \href{code.org}{\texttt{pCNProposer}} (\(\beta=0.25\)) + \href{code.org}{\texttt{pCNAccepter}}
\end{itemize}

The code is in \href{scripts/analytic.py}{\texttt{analytic.py}}.

\subsubsection{Result}
\label{sec:orgb50ef1d}

All three samplers are able to reproduce the target density \ref{fig:hist_bimodal}

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./figures/bimodal_density_combined.png}
\caption{\label{fig:hist_bimodal}
Burn-in: 1000, sample-interval: 200, samples: 500}
\end{figure}

The autocorrelation decays for all samplers: \ref{fig:ac_bimodal}. However, the pCN doens't
do nearly as well as expected. This could be the consequence of the awkward
formulation of the potential or a bad prior.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./figures/analytic_standard_rw_pCN_50_5000.png}
\caption{\label{fig:ac_bimodal}
AC of bimodal distribution. pCN takes forever to decorrelate}
\end{figure}


\subsection{Bayesian inverse problem for \(\G{u} = \langle g,u \rangle\)}
\label{sec:org0ed1feb}
For \(\G{u} = \langle g,u \rangle\) the resulting posterior under a Gaussian prior
is again a Gaussian. The model equation is
$$y = \G{u} + \eta$$
with:
\begin{itemize}
\item \(y \in \R\)
\item \(u \in \R^n\)
\item \(\eta \sim \N{0}{\gamma^2}\) for \(\gamma \in \R\)
\end{itemize}

A concrete realization with scalar \(u\):
\begin{itemize}
\item \(u = 2\)
\item \(g = 3\)
\item \(\gamma = 0.5\)
\item \(y=6.172\)
\item prior \(\N{0}{\Sigma_0=1}\)
\end{itemize}
leads to a posterior with mean
\(\mu = \frac{(\Sigma_0g)y}{\gamma^2 + \langle g, \Sigma_0g \rangle} \approx 2\),
which is what we see when we plot the result \ref{fig:stuart_21_density}.
The pCN-Sampler with \(\beta = 0.25\) had an acceptance rate of 0.567.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./figures/stuart_example_21_n=1_N=5000.png}
\caption{\label{fig:stuart_21_density}
\(N=5000, \mu \approx 2\)}
\end{figure}

For \(n>2\), the resulting posterior can not be plotted anymore. However, it is still Gaussian
with given mean \& covariance. Can just compare the analytical values to the sample values.
Verify that the error decays like \(\frac{1}{\sqrt{N}}\).
\subsection{Bayesian inverse problem for \(\G{u} = g (u + \beta u^3)\)}
\label{sec:org631c8fd}
Since the observation operator is not linear anymore, the resulting posterior is not
Gaussian in general. However, since the dimension of the input \(u\) is 1, it can
still be plotted.

The concrete realization with:
\begin{itemize}
\item \(g = [3, 1]\)
\item \(u = 0.5\)
\item \(\beta = 0\)
\item \(y= [1.672, 0.91]\)
\item \(\gamma = 0.5\)
\item \(\eta \sim \N{0}{\gamma^2 I}\)
\item prior \(\N{0}{\Sigma_0=1}\)
\end{itemize}
however leads to a Gaussian thanks to \(\beta = 0\). The mean is
\(\mu = \frac{\langle g,y \rangle}{\gamma^2 + |g|^2} \approx 0.58\). Plot: \ref{fig:stuart_22_density}

The pCN-Sampler with \(\beta = 0.25\) (different beta) had an acceptance rate of 0.576.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./figures/stuart_example_22_q=2_N=5000.png}
\caption{\label{fig:stuart_22_density}
\(N=5000, \mu \approx 0.58\)}
\end{figure}

For \(\beta \neq 0\), the resulting posterior is not a Gaussian. Still \(n=1\), so it can be
plotted. Just numerically normalize the analytical expression of the posterior?
\subsection{Geophysics example: Lorenz-96 model}
\label{sec:orgec05bfa}

\subsubsection{Based on:}
\label{sec:org0fb0770}

Lorenz, E. N. (1996). Predictability‚ÄîA problem partly solved. In Reprinted in T. N. Palmer \& R. Hagedorn (Eds.), Proceedings Seminar on
Predictability, Predictability of Weather and Climate, Cambridge UP (2006) (Vol. 1, pp. 1‚Äì18). Reading, Berkshire, UK: ECMWF.

\subsubsection{Equation}
\label{sec:org694921b}

A system of ODEs, representing the coupling between slow variables \(X\) and fast, subgrid
variables \(Y\).

$$ \dv{X_k}{t} =                 - X_{k-1}(X_{k-2} - X_{k+1}) - X_k + F - hc \bar{Y}_k $$
$$ \frac{1}{c} \dv{Y_{j,k}}{t} = -bY_{j+1,k}(Y_{j+2,k} - Y_{j-1, k}) - Y_{j,k} + \frac{h}{J}X_k$$

\begin{itemize}
\item \(X = [X_0, ..., X_{K-1}] \in \R^K\)
\item \(Y = [Y_{j, 0} | ... | Y_{j, K-1}] \in \R^{J \cross K}\) \\
\(Y_{j,k} = [Y_{0,k}, ..., Y_{J-1,k}] \in  \R^J\)
\item \(\bar{Y}_k = \frac{1}{J}\sum_j Y_{j,k}\)
\item periodic: \(X_K = X_0\), \(Y_{J,k} = Y_{0,k}\)
\item Parameters \(\Theta = [F, h, c, b]\)
\item \(h\): coupling strength
\item \(c\): relative damping
\item \(F\): external forcing of the slow variables (large scale forcing)
\item \(b\): scale of non-linear interaction of fast variables
\item \(t = 1 \Leftrightarrow 1\) day (simulation duration is given in days)
\end{itemize}

\paragraph{\(b\) or \(J\)?}
\label{sec:orgdbb37fa}
In the original paper, the equations are given in a different form, namely all
explicit occurences of \(J\) above (in the fast-slow interaction) are replaced by
\(b\). Since in both concrete realizations (1996 \& 2017) are identical and conviniently
have \(b=J=10\), the difference doesn't lead to different results for that setup.

\paragraph{Where is \(F\)?}
\label{sec:org54e57f5}
In the original paper (at least the one I found), the forcing term \(F\) for the
slow variables is missing in the equation, however it is referenced in the text,
as well as present in a simpler (only slow variables) setup.

Maybe I have a shitty version of the paper?

\paragraph{"Looking ahead" vs. "Looking back"}
\label{sec:org83c0325}
Comparing nonlinearity terms
$$-X_{k-1}(X_{k-2} - X_{k+1})$$
$$-bY_{j+1,k}(Y_{j+2,k} - Y_{j-1, k})$$
for a given \(Y_{k}\), does the "direction" of the \(Z_{k\pm 1}Z_{k\pm 2}\) (\(Z=X,Y\)) matter?

I don't think so, since the interaction with the other variable is only via point-value
and average, and the nonlinearity is periodic.

A bit more formally:
The PDE is invariant under "reversing" of the numbering:
\(Y_{j,k} \rightarrow Y_{J-j,k}\) which is the same as switching \(+ \leftrightarrow -\) in
the only "asymmetric" term.

Addendum 2 days later: Need to define more clearly what it means for the direction to
\uline{matter}. In the original paper on page 12, it is described how "active areas [..] propagate
slowly eastward", while "convective activity tends to propagate westward within the active
areas" (rephrased from paper). The paper also explicitly mentions the signs of the subscripts
in that context. So some characteristics of the solution are definitely affected.
What about the stuff we care about (statistical properties, chaotic behaviour)?

\paragraph{Initial conditions for the MCMC observation operator}
\label{sec:orgeecad4c}
In \cite{schneider_earth_2017}, they describe their algorithm as:

"The objective function for each sample is accumulated over T = 100 days, using the end state
of the previous forward integration as initial condition for the next one, without discarding
any spin-up after a parameter update."

(Disregarding the fact that for this chaotic ODE and such a long simulation time the initial
conditions are virtually irrelevant)

From a theoretical stand-point, should the observation operator \(\G{u}\) not depend \uline{only} on
\(u\), the parameters we try to estimate in the algorithm? And should it not be deterministic,
all the (stochastic) observational noise encoded in the error term \(\eta\)?

Does this changing of the IC every iteration not pose a problem for the convergence of the
algorithm?\\

Also does the spin-up they mention refer to discarding the first part of the simulation
when computing steady-state averages? It somehow doesn't make sense to talk about a
Markov-spin-up in the context of a single ODE-simulation, but ALSO it doens't make sense
to talk about averages since from my understanding the MCMC-algorithm doesn't have to
compute averages, it just looks at the final state?


\subsubsection{Properties}
\label{sec:org9bd91e9}

\begin{itemize}
\item For \(K=36\), \(J=10\) and \(\Theta = [F, h, c, b] = [10, 1, 10, 10]\) there is chaotic behaviour.

\item The nonlinearities conserve the energies within a subsystem:
\begin{itemize}
\item \(E_X = \sum_k X_k^2\)

\(\frac{1}{2} \dv{(\sum_k X_k^2)}{t} =
         \sum_k X_k \dv{X_k}{t} =
         -\sum_k (X_k X_{k-1} X_{k-2} - X_{k-1} X_{k} X_{k+1}) =
         0\),

where the last equality follows from telescoping + periodicity
\item \(E_{Y_k} = \sum_j Y_{j,k}^2\)

which follows analogously to the \(X\) -case
\end{itemize}

\item The interaction conserves the total energy:
\begin{itemize}
\item \(E_{T} = \sum_k (X_k^2 + \sum_j Y_{j,k}^2)\)

\(\frac{1}{2} \dv{E_{T}}{t} =
         \sum_k X_k \dv{X_k}{t} + \sum_j Y_{j,k} \dv{Y_j,k}{t} =
         \sum_k X_k (- \frac{hc}{J} \sum_j Y_{j,k}) + \sum_j Y_{j,k} (\frac{hc}{J} X_k) =
         \sum_k - \frac{hc}{J} X_k (\sum_j Y_{j,k} + \frac{hc}{J} X_k (\sum_j Y_{j,k})) = 
         0\)
\end{itemize}

\item In the statistical steady state, the external forcing \(F\) (as long as its positive) balances
the dampling of the linear terms.

\item Averaged quantities
\begin{itemize}
\item $$\expval{\phi} = \frac{1}{T} \int_{t_0}^{t_0 + T} \phi(t) \dd{t}$$ (or a sum over discrete values)
\item Long-term time-mean in the statistical steady state: \(\expval{\cdot}_{\infty}\)
\item \begin{equation}
\label{eqn:equilibrium_X}
  \expval{X_k^2}_\infty = F \expval{X_k}_{\infty} - hc \expval{X_k\bar{Y_k}}_\infty \forall k
\end{equation}
(multiply \(X\) -equation by \(X\), all \(X_k\) s are statistically equivalent, \(\dv{\expval{X}}{t} = 0\) in steady state)
\item \begin{equation}
\label{eqn:equilibrium_Y}
  \expval{\bar{Y_k^2}}_{\infty} = \frac{h}{J} \expval{X_k \bar{Y_k}}_{\infty} \forall k
\end{equation}
\end{itemize}
\end{itemize}

\subsubsection{Model implementation}
\label{sec:org7a7f065}
Implementing the model in python and using a locally 5-th order RK solver yields the following
results (inital conditions are just uniformly random numbers in \([0,1)\) since they don't matter
for the long-term evolution of the chaotic system):

\paragraph{Reproducting the results of the original paper}
\label{sec:orgf3652d8}
Running the setup with \(K=36, J=10, (F, h, c, b) = (10, 1, 10, 10)\) given the following states
\ref{fig:state_lorenz_30}, \ref{fig:state_lorenz_60},
which qualitatively agree with the results from Lorenz.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./figures/lorenz96_middle.png}
\caption{\label{fig:state_lorenz_30}
System after 30 days}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./figures/lorenz96_last.png}
\caption{\label{fig:state_lorenz_60}
System after 60 days}
\end{figure}

The decay of the linear term and the forcing of the slow variables balance out after reaching the
steady state, however there is a much bigger fluctuation in the energy than expected \ref{fig:lorenz_energy}.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./figures/lorenz96_energies.png}
\caption{\label{fig:lorenz_energy}
Energies in the system. \(E_X >> E_{Y_k} > 0\)}
\end{figure}

\paragraph{Equilibrium averages}
\label{sec:org098e01c}
Analysis suggests certain long-term averages to be equal in the equilibrium.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./figures/equilibrium_error_n=10.png}
\caption{\label{fig:lorenz_rmse}
RMSE for long-term averages \ref{eqn:equilibrium_X} and \ref{eqn:equilibrium_Y}. Averaged over 10 runs}
\end{figure}

\bibliographystyle{plain}
\bibliography{../papers/inverse_problems}
\end{document}
