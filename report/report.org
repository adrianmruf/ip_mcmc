#+TITLE: Markov Chain Monte Carlo for Inverse Problems


#+LATEX_HEADER: \usepackage{physics}
#+LATEX_HEADER: \usepackage{dsfont}

#+LATEX_HEADER: \newcommand{\C}{{\mathcal{C}}}
#+LATEX_HEADER: \newcommand{\I}{{\mathcal{I}}}
#+LATEX_HEADER: \newcommand{\R}{{\mathbb{R}}}
#+LATEX_HEADER: \newcommand{\G}[1]{{\mathcal{G} \left( #1 \right)}}
#+LATEX_HEADER: \newcommand{\N}[2]{\mathcal{N}\left(#1,#2\right)}

* TODO Meta                                                        :noexport:
** TODO Can I get code execution to work here for the results? (-> DIY jupyter I guess)
** TODO Can I embed svgs?
** DONE Create/Link to bibtex file
** DONE Tests for pCN prop/acc
** TODO Write down what I've done so far
*** TODO What's up with the ac of pCN?
*** DONE Write up/insert plots
** TODO Theory: What is an infinite-dimensional Gaussian?
*** Some definition about random fields blabla in cotter
*** What about BB stuart?
*** What about the internet?
** TODO Code BB Stuart Example 2.1
** TODO Code BB Stuart Example 2.2
** TODO Read Geophysics example


* Theory
** Papers
*** Stuart et al: Inverse Problems: A Bayesian Perspective [[cite:stuart_inverse_2010]]
    Theoretical Background
**** Notation
     Central equation:
     $$y = \G{u} + \eta$$
     with:
     - $y \in \R^q$: data
     - $u \in \R^n$: IC ("input to mathematical model")
     - $\G{\cdot} :\R^n \to \R^q$: observation operator
     - $\eta$: mean zero RV, observational noise (a.s. $\eta \sim \N{0}{\C}$)
*** Cotter et al: MCMC for functions [[cite:cotter_mcmc_2013]]
    Implementation, MCMC in infinite dimensions
*** Schneider et al: Earth System Modeling 2.0  [[cite:schneider_earth_2017]]
    Example for MCMC on ODE
** Small results
*** Gaussian in infinite dimensions
    This section is quite a mess, maybe you could suggest a not-too-technical introduction
    to infinite dimensional Gaussian measures?

    Wiki: Definition of Gaussian measure uses Lesbesgue measure.
    However, the Lesbesgue-Measure is not defined in an infinite-dimensional space ([[https://en.wikipedia.org/wiki/Infinite-dimensional_Lebesgue_measure][wiki]]).

    Can still define a measure to be Gaussian if we demand all push-forward measures via a
    linear functional onto $\R$ to be a Gaussian. (What about the star (E^*, L_*)
    in the wiki-article? Are they dual-spaces?) (What would be an example of that? An example
    for a linear functional on an inf-dims space given on wikipedia is integration.
    What do we integrate? How does this lead to a Gaussian?)

    How does this fit with the description in [[cite:cotter_mcmc_2013]]? -> Karhunenen-Lo√©ve
    
    What would be an example of a covariance operator in infinite dimensions?
    The Laplace-Operator operates on functions, the eigenfunctions would be $sin$, $cos$ (I think?
    This might not actually be so easy, see [[https://en.wikipedia.org/wiki/Dirichlet_eigenvalue][Dirichlet Eigenvalues]]). Are the eigenvalues
    square-summable?
    
    Anyway, when a inf-dim Gaussian is given as a KL-Expansion, an  example of a linear functional
    given as $f(u) = \langle \phi_i, u \rangle$ for $\phi_i$ an eigenfunction of $\C$, then I can see
    the push-forward definition of inf-dim Gaussians satisfied. ( $\C$ spd, so $\phi_i$ s are
    orthogonal, so we just end up with one of the KH-"components" which is given to be $\N{0}{1})$.

    The problem is not actually in $\exp(-1/2x^T\C^{-1}x)$. What about $\exp(-1/2\norm{\C^{-1/2}x})$?

    What about the terminology in [[cite:cotter_mcmc_2013]]? Absolutely continuous w.r.t a measure for
    example?

    How is the square root of an operator defined? For matrices, there seems to be a freedom in
    choosing whether $A = BB$ or $A = BB^T$ for $B = A^{1/2}$. The latter definition seems to
    be more useful when working with Cholesky factorizations (cf. https://math.stackexchange.com/questions/2767873/why-is-the-square-root-of-cholesky-decomposition-equal-to-the-lower-triangular-m),
    but for example in the wiki-article about the matrix (operator) square root (https://en.wikipedia.org/wiki/Square_root_of_a_matrix):
    "The Cholesky factorization provides another particular example of square root, which should not be confused with the unique non-negative square root."

*** Bayes' Formula & Radon-Nikodym Derivative
    Bayes' Formula is stated using the Radon-Nikodym Derivative in both [[cite:cotter_mcmc_2013]] and [[cite:stuart_inverse_2010]]:
    $$\dv{\mu}{\mu_0} \propto \text{L}(u),$$
    where $\text{L}(u)$ is the likelihood.

    Write the measures as $\dd \mu = \rho(u)\dd u$ and $\dd \mu_0 = \rho_0(u)\dd u$ with respect
    to the standard Lesbesgue measure. Then we have
    $$
    \int f(u) \rho(u) \dd u =
    \int f(u) \dd \mu(u) =
    \int f(u) \dv{\mu(u)}{\mu_0(u)} \dd \mu_0 =
    \int f(u) \dv{\mu(u)}{\mu_0(u)} \rho_0(u) \dd u,
    $$
    provided that $\dd \mu$, $\dd \mu_0$ and $f$ are nice enough (which they are since we're working
    with Gaussians). This holds for all test functions $f$, so it must hold pointwise:
    $$ \dv{\mu(u)}{\mu_0(u)} = \frac{\rho(u)}{\rho_o(u)}.$$

    Using this we recover the more familiar formulation of Bayes' formula:
    $$\frac{\rho(u)}{\rho_o(u)} \propto \text{L}(u).$$

*** Acceptance Probability for Metropolis-Hastings
    A Markov process with transition probabilities $t(y|x)$ has a stationary distribution $\pi(x)$.
    - The _existence_ of $\pi(x)$ follows from /detailed balance/:
      $$\pi(x)t(y|x) = \pi(y)t(x|y).$$
      Detailed balance is sufficient but not necessary for the existence of a stationary distribution.
    - _Uniqueness_ of $\pi(x)$ follows from the Ergodicity of the Markov process. For a Markov
      processto be Ergodic it has to:
      - not return to the same state in a fixed interval
      - reach every state from every other state in finite time
    
    The Metropolis-Hastings algorithm constructs transition probabilities $t(y|x)$ such that the
    two conditions above are satisfied and that $\pi(x) = P(x)$, where $P(x)$ is the distribution
    we want to sample from.

    Rewrite detailed balance as
    $$\frac{t(y|x)}{t(x|y)} = \frac{P(y)}{P(x)}.$$
    Split up the transition probability into proposal $g(y|x)$ and acceptance $a(y,x)$. Then detailed
    balance requires
    $$\frac{a(y,x)}{a(x,y)} = \frac{P(y)g(x|y)}{P(x)g(y|x)}.$$
    Choose
    $$a(y,x) = \min\left\{1, \frac{P(y)g(x|y)}{P(x)g(y|x)}\right\}$$
    to ensure that detailed balance is always satisfied. Choose $g(y|x)$ such that ergodicity
    is fulfilled.

    If the proposal is symmetric ($g(y|x) = g(x|y)$), then the acceptance takes the simpler form
    #+NAME: eqn:acceptance_simple
    \begin{equation}
    a(y,x) = \min\left\{1, \frac{P(y)}{P(x)}\right\}.
    \end{equation}

    Since the target distribution $P(x)$ only appears as a ratio, normalizing factors can be ignored.

*** Potential for Bayes'-MCMC when sampling from analytic distributions
    How can we use formulations of Metropolis-Hastings-MCMC algorithms designed to sample from
    posteriors when want to sample from probability distribution with an easy analytical expression?

    Algorithms for sampling from a posterior sample from
    $$\rho(u) \propto \rho_0(u) \exp(-\Phi(u)),$$
    where $\rho_0$ is the prior and $\exp(-\Phi(u))$ is the likelihood. Normally, we have an
    efficient way to compute the likelihood.

    When we have an efficient way to compute the posterior $\rho$ and we want to sample from it,
    the potential to do that is:
    $$\Phi(u) = \ln(\rho_0(u)) - \ln(\rho(u)),$$
    where an additive constant from the normalization was omitted since only potential differences
    are relevant.

    When working with a Gaussian prior $\N{0}{\C}$, the potential takes the form
    $$\Phi(u) = -\ln{\rho(u)} - \frac{1}{2} \norm{\C^{-1/2}u}^2.$$

    When inserting this into the acceptance probability for the standard random walk MCMC given
    in formula (1.2) in [[cite:cotter_mcmc_2013]], the two Gaussian-expressions cancel, as do the
    logarithm and the exponentiation, leaving the simple acceptance described in [[eqn:acceptance_simple]].

    This cancellation does not happen when using the pCN-Acceptance probablity. This could
    explain the poorer performance of pCN when directly sampling a probablity distribution.

*** Acceptance Probabilities for different MCMC Proposers
    Start from Bayes' formula and rewrite the likelyhood $\text{L}(u)$ as $\exp(-\Phi(u))$ for
    a positive scalar function $\Phi$ called the potential:
    $$\frac{\rho(u)}{\rho_o(u)} \propto \exp(\Phi(u)).$$
    Assuming our prior to be a Gaussian ($\mu_0 \sim \N{0}{\C}$).

    Then $$\rho(u) \propto \exp\left( -\Phi(u) + \frac{1}{2} \norm{C^{-1/2}u}^2 \right),$$
    since $u^T C^{-1} u = (C^{-1/2} u)^T(C^{-1/2} u) = \langle C^{-1/2}u, C^{-1/2}u \rangle = \norm{C^{-1/2} u}^2$,
    where in the first equality we used $C$ being symmetric.

    This is formula (1.2) in [[cite:cotter_mcmc_2013]] and is used in the acceptance probability for
    the standard random walk (see also [[Acceptance Probability for Metropolis-Hastings][Acceptance Probability for Metropolis-Hastings]])

    $\C^{-1/2}u$ makes problems in infinite dimensions.

    Todo: Why exactly is the second term (from the prior) cancelled when doing pCN?
*** Different formulations of multivariate Gaussians
    Is an RV $\xi \sim \N{0}{C}$ distributed the same as $C^{1/2}\xi_0$, with $\xi_0 \sim \N{0}{\I}$?

    From wikipedia: Affine transformation $Y = c + BX$ for $X \sim \N{\mu}{\Sigma}$ is also a Gaussian
    $Y \sim \N{c + B\mu}{B\Sigma B^T}$. In our case $X \sim \N{0}{\I}$, so $Y \sim \N{0}{C^{1/2}\I {C^{1/2}}^{T}} = \N{0}{C}$,
    since the covariance matrix is positive definite, which means it's square root is also positive definite
    and thus symmetric.

    On second thought, it also follows straight from the definition:
    $$
      \mathbf{X} \sim \N{\mu}{\Sigma}
      \Leftrightarrow
      \exists \mu \in \R^k, A \in \R^{k \cross l}
        \text{ s.t. }
        \mathbf{X} = \mu + A\mathbf{Z}
        \text{ with } \mathbf{Z}_n \sim \N{0}{1} \text{ i.i.d}
    $$
    where $\Sigma = AA^T$.

* Implementation
** Framework/Package Structure
   The framework is designed to support an easy use case:
   #+BEGIN_SRC python
   proposer = StandardRWProposer(beta=0.25, dims=1)
   accepter = AnalyticAccepter(my_distribution)
   rng = np.random.default_rng(42)
   sampler = MCMCSampler(rw_proposer, accepter, rng)

   samples = sampler.run(x_0=0, n_samples=1000)
   #+END_SRC

   There is only one source of randomness, shared among all classes and supplied by the user.
   This facilitates reproducability.

   Tests are done with ~pytest~.
*** Distributions
    A class for implementing probability distributions.
    #+BEGIN_SRC python
    class DistributionBase(ABC):
        @abstractmethod
        def sample(self, rng):
            """Return a point sampled from this distribution"""
            ...
    #+END_SRC
    
    The most important realisation is the ~GaussianDistribution~, used
    in the proposers.

    #+BEGIN_SRC python    
    class GaussianDistribution(DistributionBase):
        def __init__(self, mean=0, covariance=1):
            ...

        def sample(self, rng):
            ...

        def apply_covariance(self, x):
            ...

        def apply_sqrt_covariance(self, x):
            ...

        def apply_precision(self, x):
            ...

        def apply_sqrt_precision(self, x):
            ...
    #+END_SRC

    The design of this class is based on the implementation in [[http://muq.mit.edu/master-muq2-docs/CrankNicolson_8py_source.html][muq2]]. The ~precision~ / ~sqrt_precision~
    is implemented through a Cholesky decomposition, computed in the constructor. This makes
    applying them pretty fast ($\mathcal{O}(n^2)$).

    At the moment the there is one class for both scalar and multivariate Gaussians. This
    introduces some overhead as it has to work with both ~float~ and ~np.array~. Maybe two
    seperate classes would be better.

    Also, maybe there is a need to implement a Gaussian using the Karhunen-Lo√©ve-Expansion?
*** Potentials
    A class for implementing the potential resulting from rewriting the likelihood as
    $$\text{L}(u) = \exp(- \Phi(u)).$$
   
    #+BEGIN_SRC python
    class PotentialBase(ABC):
    """
        Potential used to express the likelihood;
        d mu(u; y) / d mu_0(u) \propto L(u; y)
        Write L(u; y) as exp(-potential(u; y))
        """
        @abstractmethod
        def __call__(self, u):
            ...

        @abstractmethod
        def exp_minus_potential(self, u):
            ...
    #+END_SRC

    The two functions return $\Phi(u)$ and $\exp(-\Phi(u))$ respectively. Depending on the
    concrete potential, one or the other is easier to compute.

    Potentials are used in the accepters to decide the relative weight of different configurations.
    There, the ~PotentialBase.exp_minus_potential~ is used.
**** AnalyticPotential

     This potential is used when sampling from an analytically computable probability distribution,
     i.e. a known posterior. In this case
     $$\exp(-\Phi(u)) = \frac{\rho(u)}{\rho_0(u)},$$
     see [[Potential for Bayes'-MCMC when sampling from analytic distributions]]
**** EvolutionPotential

     This potential results when sampling from the model-equation
     $$y = \G{u} + \eta,$$
     with $\eta \sim \rho$. The resulting potential can be computed as
     $$\exp(-\Phi(u)) = \rho(y - \G{u}).$$

*** Proposers

    Propose a new state $v$ based on the current one $u$.

    #+BEGIN_SRC python
    class ProposerBase(ABC):
        @abstractmethod
        def __call__(self, u, rng):
            ...
    #+END_SRC

**** StandardRWProposer

     Propose a new state as
     $$v = u + \sqrt{2\delta} \xi,$$
     with either $\xi \sim \N{0}{\I}$ or $\xi \sim \N{0}{\C}$ (see section 4.2 in [[cite:cotter_mcmc_2013]]).

     This leads to a well-defined algorithm in finite dimensions.
     This is not the case when working on functions (as described in section 6.3 in [[cite:cotter_mcmc_2013]])

**** pCNProposer

     Propose a new state as
     $$v = \sqrt{1-\beta^2} u + \beta \xi,$$
     with $\xi \sim \N{0}{\C}$ and $\beta = \frac{8\delta}{(2+\delta)^2} \in [0,1]$
     (see formula (4.8) in [[cite:cotter_mcmc_2013]]).

     This approach leads to an improved algorithm (quicker decorrelation in finite dimensions,
     nicer properties for infinite dimensions)(see sections 6.2 + 6.3 in [[cite:cotter_mcmc_2013]]).

     The wikipedia-article on the Cholesky-factorization mentions the use-case of obtaining a
     correlated sample from an uncorrelated one by the Cholesky-factor. This is not implemented here.
*** Accepters

    Given a current state $u$ and a proposed state $v$, decide if the new state is accepted or rejected.

    For sampling from a distribution $P(x)$, the acceptance probability for a symmetric proposal is
    $a = \text{min}\{1, \frac{P(v)}{P(u)}\}$
    (see [[Acceptance Probability for Metropolis-Hastings]])

    #+BEGIN_SRC python
    class ProbabilisticAccepter(AccepterBase):
        def __call__(self, u, v, rng):
            """Return True if v is accepted"""
            a = self.accept_probability(u, v)
            return a > rng.random()

        @abstractmethod
        def accept_probability(self, u, v):
            ...
    #+END_SRC

**** AnalyticAccepter

     Used when there is an analytic expression of the desired distribution.

    #+BEGIN_SRC python
    class AnalyticAccepter(ProbabilisticAccepter):
        def accept_probability(self, u, v):
            return self.rho(v) / self.rho(u)
    #+END_SRC

**** StandardRWAccepter

     Based on formula (1.2) in [[cite:cotter_mcmc_2013]]:
     $$a = \text{min}\{1, \exp(I(u) - I(v))\},$$ with
     $$I(u) = \Phi(u) + \frac{1}{2}\norm{\C^{-1/2}u}^2$$.

     See also [[Acceptance Probabilities for different MCMC Proposers]].

**** pCNAccepter

     Works together with the [[pCNProposer][pCNProposer]] to achieve the simpler expression for the acceptance
     $$a = \text{min}\{1, \exp(\Phi(u) - \Phi(v))\}.$$

**** CountedAccepter

     Stores and forwards calls to an "actual" accepter. Counts calls and accepts and is used for
     calculating the acceptance ratio.
    
*** Sampler

    The structure of the sampler is quite simple, since it can rely heavily on the functionality
    provided by the Proposers and Accepters.

    #+BEGIN_SRC python
    class MCMCSampler:
        def __init__(self, proposal, acceptance, rng):
            ...

        def run(self, u_0, n_samples, burn_in=1000, sample_interval=200):
            ...

        def _step(self, u, rng):
            ...
    #+END_SRC

** Results
*** Analytic sampling from a bimodal Gaussian
**** Setup

     Attempting to recreate the "Computational Illustration" from [[cite:cotter_mcmc_2013]]. They use,
     among other algorithms, pCN to sample from a 1-D bimodal Gaussian
     $$\rho \propto (\N{3}{1} + \N{-3}{1}) \mathds{1}_{[-10,10]}.$$
     Since the density estimation framework for a known distribution is not quite clear to me from
     the paper, I don't expect to perfectly replicate their results.

     They use a formulation of the prior based on the Karhunen-Lo√©ve Expansion that doesn't make
     sense to me in the 1-D setting (how do I sum infinite eigenfunctions of a scalar?).

     The potential for density estimation described in section is also not clear to me (maybe for
     a similar reason? What is $u$ in the density estimate case?).

     I ended up using a normal $\N{0}{1}$ as a prior and the potential described [[Potential for Bayes'-MCMC when sampling from analytic distributions][before]], and
     compared the following samplers:
     - (1) [[StandardRWProposer][~StandardRWProposer~]] ($\delta=0.25$) + [[AnalyticAccepter][~AnalyticAccepter~]]
     - (2) [[StandardRWProposer][~StandardRWProposer~]] ($\delta=0.25$) + [[StandardRWAccepter][~StandardRWAccepter~]] 
     - (3) [[pCNProposer][~pCNProposer~]] ($\beta=0.25$) + [[pCNAccepter][~pCNAccepter~]] 

     The code is in [[file:scripts/analytic.py][~analytic.py~]].

**** Result

     All three samplers are able to reproduce the target density [[fig:hist_analytic]] [[fig:hist_rw]] [[fig:hist_rw]].

     #+CAPTION: analytic
     #+NAME: fig:hist_analytic
     [[./figures/analytic_bimodal_density.png]]
     #+CAPTION: standard rw
     #+NAME: fig:hist_rw
     [[./figures/standard_bimodal_density.png]]
     #+CAPTION: pCN
     #+NAME: fig:hist_pCN
     [[./figures/pCN_bimodal_density.png]]

     The autocorrelation decays for all samplers: [[fig:ac_normal]], [[fig:ac_bimodal]]. However, the pCN doens't
     do nearly as well as expected. This could be the consequence of the awkward
     formulation of the potential or a bad prior.

     A peculiar thing about the decorrelation of the pCN sampling process is that
     it somehow is tied to the number of samples, compare [[fig:ac_pCN_1000]] and [[fig:ac_pCN_2000]].
     Is this a bug or a misunderstanding of the autocorrelation function? 

     #+CAPTION: AC of standard normal. All samplers decorrelate quickly
     #+NAME: fig:ac_normal
     [[./figures/analytic_standard_rw_pCN_normal.png]]

     #+CAPTION: AC of bimodal distribution. pCN takes forever to decorrelate
     #+NAME: fig:ac_bimodal
     [[./figures/analytic_standard_rw_pCN_bimodal_20000.png]]

     #+CAPTION: AC of bimodal distribution.
     #+NAME: fig:ac_pCN_1000
     [[./figures/analytic_standard_rw_pCN_bimodal_1000.png]]

     #+CAPTION: AC of bimodal distribution.
     #+NAME: fig:ac_pCN_2000
     [[./figures/analytic_standard_rw_pCN_bimodal_2000.png]]
     

*** Bayesian inverse problem for $\G{u} = \langle g,u \rangle$
    For $\G{u} = \langle g,u \rangle$ the resulting posterior under a Gaussian prior
    is again a Gaussian. The model equation is
    $$y = \G{u} + \eta$$
    with:
    - $y \in \R$
    - $u \in \R^n$
    - $\eta \sim \N{0}{\gamma^2}$ for $\gamma \in \R$

    A concrete realization with scalar $u$:
    - $u = 2$
    - $g = 3$
    - $\gamma = 0.5$
    - $y=6.172$
    - prior $\N{0}{\Sigma_0=1}$
    leads to a posterior with mean
    $\mu = \frac{(\Sigma_0g)y}{\gamma^2 + \langle g, \Sigma_0g \rangle} \approx 2$,
    which is what we see when we plot the result [[fig:stuart_21_density]].
    The pCN-Sampler with $\beta = 0.25$ had an acceptance rate of 0.567.
    
    #+CAPTION: $N=5000, \mu \approx 2$
    #+NAME: fig:stuart_21_density
    [[./figures/stuart_example_21_n=1_N=5000.png]]

    For $n>2$, the resulting posterior can not be plotted anymore. However, it is still Gaussian
    with given mean & covariance. Can just compare the analytical values to the sample values.
    Verify that the error decays like $\frac{1}{\sqrt{N}}$.
*** Bayesian inverse problem for $\G{u} = g (u + \beta u^3)$
    Since the observation operator is not linear anymore, the resulting posterior is not
    Gaussian in general. However, since the dimension of the input $u$ is 1, it can
    still be plotted.

    The concrete realization with:
    - $g = [3, 1]$
    - $u = 0.5$
    - $\beta = 0$
    - $y= [1.672, 0.91]$
    - $\gamma = 0.5$
    - $\eta \sim \N{0}{\gamma^2 I}$
    - prior $\N{0}{\Sigma_0=1}$
    however leads to a Gaussian thanks to $\beta = 0$. The mean is
    $\mu = \frac{\langle g,y \rangle}{\gamma^2 + |g|^2} \approx 0.58$. Plot: [[fig:stuart_22_density]]

    The pCN-Sampler with $\beta = 0.25$ (different beta) had an acceptance rate of 0.576.

    #+CAPTION: $N=5000, \mu \approx 0.58$
    #+NAME: fig:stuart_22_density
    [[./figures/stuart_example_22_q=2_N=5000.png]]

    For $\beta \neq 0$, the resulting posterior is not a Gaussian. Still $n=1$, so it can be
    plotted. Just numerically normalize the analytical expression of the posterior?
*** Geophysics example: Lorenz-96 model
**** Based on:

     Lorenz, E. N. (1996). Predictability‚ÄîA problem partly solved. In Reprinted in T. N. Palmer & R. Hagedorn (Eds.), Proceedings Seminar on
     Predictability, Predictability of Weather and Climate, Cambridge UP (2006) (Vol. 1, pp. 1‚Äì18). Reading, Berkshire, UK: ECMWF.
**** Equation

     A system of ODEs, representing the coupling between slow variables $X$ and fast, subgrid
     variables $Y$.

     $$ \dv{X_k}{t} =                 - X_{k-1}(X_{k-2} - X_{k+1}) - X_k + F - hc \bar{Y}_k $$
     $$ \frac{1}{c} \dv{Y_{j,k}}{t} = -bY_{j+1,k}(Y_{j-2,k} - Y_{j+1, k}) - Y_{j,k} + \frac{h}{J}X_k$$

     - $X = [X_0, ..., X_{K-1}] \in \R^K$
     - $Y = [Y_{j, 0} | ... | Y_{j, K-1}] \in \R^{J \cross K}$ \\
       $Y_{j,k} = [Y_{0,k}, ..., Y_{J-1,k}] \in  \R^J$
     - $\bar{Y}_k = \frac{1}{J}\sum_j Y_{j,k}$
     - periodic: $X_K = X_0$, $Y_{J,k} = Y_{0,k}$
     - Parameters $\Theta = [F, h, c, b]$
     - $h$: coupling strength
     - $c$: relative damping
     - $F$: external forcing of the slow variables (large scale forcing)
     - $b$: scale of non-linear interaction of fast variables
     - $t = 1 \Leftrightarrow 1$ day (simulation duration is given in days)

**** Properties

     - For $K=36$, $J=10$ and $\Theta = [F, h, c, b] = [10, 1, 10, 10]$ there is chaotic behaviour.

     - The nonlinearities conserve the energies within a subsystem: (show!)
       - $E_X = \sum_k X_k^2$
       - $E_{Y_k} = \sum_j Y_{j,k}^2$

     - The interaction conserves the total energy: (show!)
       - $E_{T} = \sum_k (X_k^2 + \sum_j Y_{j,k}^2)$

     - In the statistical steady state, the external forcing $F$ (as long as its positive) balances
       the dampling of the linear terms.

     - Averaged quantities
       - $\expval{\phi} = \frac{1}{T} \int_{t_0}^{t_0 + T} \phi(t) \dd{t}$ (or a sum over discrete values)
       - Long-term time-mean in the statistical steady state: $\expval{\cdot}_{\infty}$
       - $\expval{X^2}_\infty = F \expval{X}_{\infty} - hc \expval{X\bar{Y}}_\infty$ $\forall k$ \\
         (multiply $X$ -equation by $X$, all $X_k$ s are statistically equivalent, $\dv{X}{t} = 0$ in steady state)
       - $\expval{\bar{Y^2}}_{\infty} = \frac{h}{J} \expval{X \bar{Y}}_{\infty}$


 #+BIBLIOGRAPHY: ../papers/inverse_problems plain
