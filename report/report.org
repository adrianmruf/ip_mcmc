#+TITLE: Markov Chain Monte Carlo for Inverse Problems

#+OPTIONS: H:5

#+INCLUDE: latex_header.org

#+INCLUDE: theory.org

#+INCLUDE: code.org

#+INCLUDE: results.org

#+BIBLIOGRAPHY: ../papers/inverse_problems plain

* TODO Meta                                                        :noexport:
** TODO Can I get code execution to work here for the results? (-> DIY jupyter I guess)
** TODO Can I embed svgs?
** DONE Create/Link to bibtex file
** DONE Tests for pCN prop/acc
** DONE Write down what I've done so far
*** TODO What's up with the ac of pCN?
*** DONE Write up/insert plots
** TODO Theory: What is an infinite-dimensional Gaussian?
*** Some definition about random fields blabla in cotter
*** What about BB stuart?
*** What about the internet?
** DONE Code BB Stuart Example 2.1
** DONE Code BB Stuart Example 2.2
** DONE Read Lorenz96
** DONE Implement Lorenz96
** DONE Report on Lorenz96 model
** DONE Do MCMC on Lorenz96
*** DONE dont make c uncertain
*** DONE nicer priors
*** maybe even make the system even smaller (J=1 (0?) -> no fast dynamics)
*** DONE AC plot to prove that MCMC works
** DONE Report on LorenzMCMC
*** DONE Write about how the "ergodic properties" of the Lorenz system actually help you
    Time average over long simulation = ensemble average
    Get a lot of information about the system in a single run
    The intitial condititions of the evolution operator are meaningless
*** Combine plots
    Probably easier to do with matplotlib
**** DONE 3 lorenz states
**** DONE 3 density plots F, h, b
*** DONE Correct model setup
*** DONE Time-averages are just np.mean, but the RK-integrator does non-uniform timesteps
** DONE AC Section
*** DONE Side-by-side plot: [0,2] vs [-1, 1] vs [0, 2] - mean
** TODO Burgers
*** Investigate
**** How much burn-in is enough: check that average of values is in steady state
**** Ground truth: do mcmc on fine grid O(500) and a lot of samples
**** Wasserstein-distance convergence when grid converges
*** TODO Write report
**** DONE Compare betas: b=0.01, b=0.15, b=0.5
**** DONE deta = 0.045 was actually wrong, it should be 0.01125
*** Autocorrelation still a big shady IMO
*** TODO Write a chain-analysis script (add acceptance computation)
*** TODO Wasserstein convergence over chain-length
    Situation: Taking one long chain (10000), split it up in (5000, 2500, 1250,...)
    (non-overlapping or overlapping, doesn't affect result)
    Then do Wasserstein-distance vs. delta function at the ground truth
    Result: opposite of convergence, the same for 3D case and just delta_1

    Probably because there are some "excursions" of size O(100) samples which
    really influence the distribution negatively. The longer the chain, the more
    excursions, but because the chains are so short (longest one has like 250 samples)
    the excursion don't really get corrected/washed out.

    Solution: Just take the average. But average over what?
    Samples: lol
    Generated densities: The same as just making all chains longer
    Distance: no.
    
*** Investigate effect of mesh-spacing

*** Stuff that could be done
**** investigate acceptance ratio and ac as function of prop_beta
**** investigate goodness of fit as function of measurement placement
***** metric for goodness of fit?
**** investigate different samplers (pCN vs ordinary RW)
***** as far as I understand the paper, pCN is good because it works in high dimensions
****** proposer-measure doesn't become degenerate or smth
**** Convergence study over grid refinement or smth?
**** ?
** Report: Lessons learned
   - Distributions was a good idea to make class,
     however it evolved more into a wrapper for scipy.stats pdf
     -> so what's the lesson learned?
   - logpdf is very important (conjectured)

