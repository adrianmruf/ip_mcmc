* Model
** Burgers' Equation

   We consider the Burgers' equation

   #+NAME: eqn:burgers
   \begin{equation}
     w_t + \left( \frac{w^2}{2} \right)_x = 0
   \end{equation}

   in the domain $(x, t) \in [-1, 1] \cross [0,1]$.

** Riemann Problem

   The following family of initial conditions:

   #+NAME: eqn:perturbed_riemann
   \begin{equation}
     w(x, 0) =
     \begin{cases}
       1 + \delta_1 & \text{if } x < \sigma_0 \\
       \delta_2     & \text{if } x > \sigma_0    
     \end{cases}
   \end{equation}

   can be parametrized by the vector $u = [\delta_1, \delta_2, \sigma_0] \in \R^3$.
   As long as $\norm{u} \ll 1$ we can expect $w$ to behave very similarly to the
   usual 1,0-Riemann problem (in particular the location of the shock at $t=1$ will be
   close to $x=0.5$).

** Discretization

   Blabla
* MCMC

** Hyperparameter tuning

   To optimize hyperparameters, namely choose the step-size (pCN: \beta, RW: \delta) such that for a given chain length
   as many samples as possible can be used for estimation, the properties of the chain
   should be well defined and computable.

   The two important characteristics are the burn-in $b$ and the decorrelation time $\tau_0$.
   Given these values for a chain of length $N$, the number of usable samples $M$ is

   $$ M = \frac{N - b}{\tau_0} $$ 

*** Burn-In $b$

    The most fruitful approach seems to be to visually inspect the evolution of the
    parameter values and roughly decide when a steady-state is reached. This works nicely
    as long as the step-size is not too small and we actually reach a steady state.

    A more formal approach would require to actually define a criterion for the parameter
    evolution in the chain that indicates when the steady state is reached. This is challenging,
    especially when no knowledge of the underlying values (ground truth) is used, and when
    the criterion should be valid for a wide range of step-sizes.

    #+CAPTION: Chain evolution during sampling with $\beta=0.1$. Visually it seems the the steady-state is reached after around 1000 samples.
    #+NAME: fig:burgers_burn-in
    [[./figures/burgers_chain_report.png]]

*** Decorrelation time $\tau_0$

    After the burn-in is discarded from the original chain, the lag where the autocorrelation
    function first equals 0 gives the number of samples after which they become decorrelated
    [fn:decorr].

    Since after burn-in the chain is in the statistical steady state, the autocorrelation
    function is the same, regardless of which interval of the chain is investigated (this
    is not the case before discarding the burn-in).

[fn:decorr] A different, more involved criterion would be to define the decorrelation time
as the integral over the autocorrelation function; $\Theta$


    #+CAPTION: Autocorrelation function during sampling with $\beta=0.1$. For this chain, $\tau_0 \approx 30$.
    #+NAME: fig:burgers_decorr_time
    [[./figures/burgers_ac_report.png]]

*** Step size \beta

    Given a way to compute $b$ and $\tau_0$, the optimal $\beta^*$ can be found as

    $$\beta^* = \argmax M(\beta)$$

    for a given chain length $N$ (which is usually constrained by computational resources).

    Generally, a bigger value of $\beta$ will result in bigger steps proposed during the
    MCMC steps. This results in a shorter burn-in at the expense of more declined steps during the
    steady state, which results in longer decorrelation times.

    Everything here also applies to \delta, the step-size for the random-walk-MCMC algorithm.
    \beta and \delta are related through $\beta^2 = \frac{8 \delta}{(2 + \delta)^2}$.


** Setup

   As usual, we sample based on the equation

   $$y = \G{u} + \eta$$

   with:
   - $y \in \R^q$: measurements obtained from a simulation of the ground truth
   - $u \in \R^n$: vector parametrizing the pertubations to the Riemann initial conditions
   - $\G{\cdot} :\R^n \to \R^q$: observation operator, measurements on the final state of the Riemann problem
   - $\eta \sim \N{0}{\gamma^2 \I_q}$: assumed observational noise [fn:variables]

   Stuart et. al. describe some cases in [[cite:stuart_inverse_2010]] (Theorem 2.17) for overdetermined 
   problems ($q > n$), where the posterior converges to a Dirac measure when $\gamma \to 0$.
   This however only applies to linear invertible observational maps, which is definitely not the
   case here. However for well-placed measurements we can definitely expect a sharp posterior.

[fn:variables] I took the liberty of renaming variables to match more closely Stuart's notation
[[cite:stuart_inverse_2010]] and avoid collisions such as multiple occurences of $\beta$.

*** Observation operator $\G{u}$

    We use the FVM to evolve the Riemann intial conditions [[eqn:perturbed_riemann]] $w_u(x, 0)$
    until $T=1$ and then measure the resulting state around certain measurement points:

    \begin{equation}
    L_i(w) = 10 \int_{x_i - 0.05}^{x_i + 0.05} w(x, 1) \dd x
    \end{equation}

    with $1 \leq i \leq 5$ and $x_1 = -0.5$, $x_2= -0.25$, $x_3 = 0.25$, $x_4 = 0.5$, $x_5 = 0.75$.

    The observation operator is then:

    $$yeah how do you write this out lol$$

**** Placement of measurements

     The choice of the $x_i$ s is crucial. If the shock is not contained in the measurement
     interval around and $x_i$, the Markov chain has no chance of determining the initial
     shock location $\sigma_0$ any more accurately than the spacing between measurements.

     Conversely, if the measurement interval is large enough, a single measurement around the
     shock gives enough information to determine all three parameters $\delta_1, \delta_2, \sigma_0$
     simultaneously, provided the Markov chain "finds" to correct parameter configuration to place the
     shock in the measurement interval.

*** Ground truth measurements $y$

    $y$ is obtained by applying the observation operator to the ground truth $u^*$.
    $$u^* = [\delta_1^*, \delta_2^*, \sigma_0^*] = [0.025, -0.025, -0.02]$$

   #+CAPTION: Setup for the MCMC experiment. The values for $w$ at $T=1$, once for the unperturbed Riemann problem, once for the ground truth of the simulation $u^*$. The green rectangles are the measurement intvervals of the observation operator : $\int_{x_i - 0.05}^{x_i + 0.05} w(x,1)\dd x$, $x_i \in \{ -0.5, -0.25, 0.25, 0.5, 0.75 \}$.
   #+NAME: fig:burgers_setup
   [[./figures/burgers_setup.png]]

*** Noise

    $\eta \sim \N{0}{\gamma^2 \I_5}$ with $\gamma = 0.05$.

*** Prior

    $\nu \sim \N{u_p}{\varphi^2 \I_3}$, with
    - $u_p = [1.5, 0.25, -0.5]$,
      which corresponds to
      - $\delta_1^p = 1.5$
      - $\delta_2^p = 0.25$
      - $\sigma_0^p = -0.5$
    - $\varphi = 0.25$

** Result

*** Investigating concrete values of \beta

    Three concrete values for \beta are investigated closer; $\beta_1 = 0.01$, $\beta_2 = 0.15$
    and $\beta_3 = 0.5$. These values were chosen since they correspond to three significantly
    different behaviours of the Markov chain.

    The pCN-proposer computes prospective new states as

    $$v = \sqrt{1-\beta^2} u + \beta \xi$$

    with $\xi \sim \N{0}{\Sigma_0}$, where $\Sigma_0$ is the covariance of the prior. Ignoring the
    scaling of the current state, a characteristic step-size can be said to be $s = \beta \Sigma_0^{-\frac{1}{2}}$,
    which in the case of $\Sigma_0 = \gamma^2 \I_q$ takes the simpler form

    #+NAME: eqn:char_step
    \begin{equation}
      s = \beta \gamma
    \end{equation}
    
    It is interesting to compare this value to other numbers in the system.

    Comparing $s$ to the distance between the prior-mean and the ground truth (namely for \delta_1,
    for which this distance is largest) gives us a rough idea of the length of the burn-in we
    can expect.

    Conversely, the ratio betwenn $s$ and the measurement interval can indicate how high the acceptance
    ratio in the steady state might be.
    The idea is that if the stepsize is much larger than the measurement interval, proposed states will
    likely move the shock outside of the measurement interval and are thus often rejected. (This
    relationship is admittedly not so simple, since a large change in \sigma_0 can be compensated
    by an adjustment in a \delta)

**** \beta = 0.01

     This very small value of beta gives a characteristic step size $s = 0.0025$. Moving uniformly from
     the prior-mean $\delta_1^p = 1.5$ to the ground truth $\delta_1^* = 0.025$ is expected to take
     around 600 steps.

     What we see in the actual chain evolution is quite different, the steps taken by are so small that
     the chain gets stuck in a local minimum and places the shock in the wrong measurement interval, even
     after 5000 steps. It can be argued that this is all part of the burn-in, and indeed also chains with
     a larger \beta sometimes spend some iterations with the shock-value in the completely wrong location.
    
     #+CAPTION: Evolution of the chain with \beta = 0.01. The small step size results in getting stuck in a local minimum, placing the around x=0.25 instead of x=0.5.
     #+NAME: fig:burgers_chain_01
     [[./figures/burgers_pCN_n=5000_b=0.01_chain_report.png]]

**** \beta = 0.5

     This large value of \beta results in stagnant behaviour in the steady state. Only very few moves
     are accepted,  so the sampling interval has to be chosen very large to get adequately decorrelated
     samples (the autocorrelation function doesn't reach 0 until well after 100 samples).
     This is not too surprising when comparing the measurement interval of 0.1 around $x=0.5$ with
     the step-size $s = 0.125$.

     #+CAPTION: Evolution of the chain with \beta = 0.5. After the burn-in, very few moves are accepted, resulting in a long decorrelation time (even longer than written on the figure).
     #+NAME: fig:burgers_chain_5
     [[./figures/burgers_pCN_n=5000_b=0.5_chain_report.png]]

**** \beta = 0.15

     With this value of \beta we get a "healthy" behaviour of the chain: the steps are large enough
     to finish the burn-in in a reasonable time, while still being small enough to explore phase-space
     around a favourable state. The characteristic step size $s = 0.375$ reflects that fact.

     However, the region which we explore in the steady-state is still quite large, result in
     not very sharp posteriors. If sharper posteriors are needed, the value of \beta should
     be decreased, while making sure the burn-in doesn't take too long.
     An adaptive (decreasing whith chain length) value of \beta could help here.


     #+CAPTION: Evolution of the chain with \beta = 0.15. After the burn-in, the phase space around the ground-truth is explored nicely. Interesting is the small "excursion" around step 4800.
     #+NAME: fig:burgers_chain_15
     [[./figures/burgers_pCN_n=5000_b=0.15_chain_report.png]]

**** Variable \delta

     The idea to have a variable step-size (usually monotonically decreasing) to reap the benefits
     of both worlds (short burn-in and quick decorrelation in the steady state) is frequently used
     in optimization. There it is called /simulated annealing/, based on an analogy to tempering metals.
     The ground state (minimizing the free energy) of the system has favorable mechanical properties
     and is reached by letting the metal cool slowly. This process is "simulated" by decreasing the
     step-size of the Markov chain, which in a physical system corresponds to lowering the temperature.
     This procedure can be very successful at finding global minima of challenging objective functions.

     Here, we chose a linearly decreasing step-size during burn-in, which is kept constant after. The
     results look promising and result in the best-performing chain.

    #+CAPTION: Evolution of the chain with a random walk proposal and a piecewise-linear \delta, starting at $\delta_s = 0.1$ and decreasing to $\delta_e = 0.001$ during burn-in.
    #+NAME: fig:burgers_delta_pwl
    [[./figures/burgers_RW_n=5000_b=pwl_0.1_0.001_250_chain_report.png]]

*** pCN vs ordinary random Walk

    The pCN proposer generates new states as

    $$v = \sqrt{1-\beta^2} u + \beta \xi,$$

    while the ordinary random walk proposer does

    $$x = u + \sqrt{2\delta}\xi$$

    with $\xi \sim \N{0}{\Sigma_0}$.

    Equating the stepsize $s$ gives $\delta = 0.01125$ being equivalent to $\beta = 0.15$.
    The chain seems pretty comparable, but the burn-in is noticably shorter.
    This can be attributed to the scaling of the current state $\sqrt{1-\beta^2}$, which "pulls"
    the proposed state towards the prior mean.

    #+CAPTION: Evolution of the chain with a random walk proposal and $\delta = 0.01125$
    #+NAME: fig:burgers_delta_0
    [[./figures/burgers_RW_n=5000_delta=0.01125_chain_report.png]]

*** Posterior estimates

    #+CAPTION: Posterior densities, taken from the pCN-chain shown above with $\beta = 0.15$, burn-in 500 and sampling interval 25.
    #+NAME: fig:burgers_densities_15
    [[./figures/burgers_pCN_n=5000_b=0.15_densities_report.png]]
    
    #+CAPTION: Posterior densities, taken from the RW-chain shown above with $\delta = 0.01125$, burn-in 500 and sampling interval 25.
    #+NAME: fig:burgers_densities_delta_001125
    [[./figures/burgers_posterior_RW_const.png]]


    #+CAPTION: Posterior densities, taken from the RW-chain shown above with piecewise-linear $\delta = 0.1 \to 0.001$, burn-in 250 and sampling interval 20.
    #+NAME: fig:burgers_densities_delta_pwl
    [[./figures/burgers_posterior_pwl.png]]
