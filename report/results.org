* Results
** Analytic sampling from a bimodal Gaussian
*** Setup

    Attempting to recreate the "Computational Illustration" from [[cite:cotter_mcmc_2013]]. They use,
    among other algorithms, pCN to sample from a 1-D bimodal Gaussian
    $$\rho \propto (\N{3}{1} + \N{-3}{1}) \mathds{1}_{[-10,10]}.$$
    Since the density estimation framework for a known distribution is not quite clear to me from
    the paper, I don't expect to perfectly replicate their results.

    They use a formulation of the prior based on the Karhunen-Lo√©ve Expansion that doesn't make
    sense to me in the 1-D setting (how do I sum infinite eigenfunctions of a scalar?).

    The potential for density estimation described in section is also not clear to me (maybe for
    a similar reason? What is $u$ in the density estimate case?).

    I ended up using a normal $\N{0}{1}$ as a prior and the potential described [[Potential for Bayes'-MCMC when sampling from analytic distributions][before]], and
    compared the following samplers:
    - (1) [[file:code.org::StandardRWProposer][~StandardRWProposer~]] ($\delta=0.25$) + [[file:code.org::AnalyticAccepter][~AnalyticAccepter~]]
    - (2) [[file:code.org::StandardRWProposer][~StandardRWProposer~]] ($\delta=0.25$) + [[file:code.org::StandardRWAccepter][~StandardRWAccepter~]]
    - (3) [[file:code.org::pCNProposer][~pCNProposer~]] ($\beta=0.25$) + [[file:code.org::pCNAccepter][~pCNAccepter~]]

    The code is in [[file:scripts/analytic.py][~analytic.py~]].

*** Result

    All three samplers are able to reproduce the target density [[fig:hist_bimodal]]

    #+CAPTION: Burn-in: 1000, sample-interval: 200, samples: 500
    #+NAME: fig:hist_bimodal
    [[./figures/bimodal_density_combined.png]]

    The autocorrelation of the pCN sampler doesn't decay nearly as well as expected.
    This could be the consequence of the awkward formulation of the potential or a bad prior.

** Bayesian inverse problem for $\G{u} = \langle g,u \rangle$
   For $\G{u} = \langle g,u \rangle$ the resulting posterior under a Gaussian prior
   is again a Gaussian. The model equation is
   $$y = \G{u} + \eta$$
   with:
   - $y \in \R$
   - $u \in \R^n$
   - $\eta \sim \N{0}{\gamma^2}$ for $\gamma \in \R$

   A concrete realization with scalar $u$:
   - Ground truth $u^* = 1$
   - $g = 1$
   - $\gamma = 0.1$
   - $y = 1.012$
   - prior $\N{0}{\Sigma_0=(0.5)^2}$
   leads to a posterior givne by [[fig:stuart_21_density]].

   #+CAPTION: Posterior sampled from a chain with length 1'000'000, using a standard random walk sampler.
   #+NAME: fig:stuart_21_density
   [[./figures/Stuart_21.png]]

*** Posterior mean is off

    According to Stuart, the posterior is a Gaussian with

    - mean: $m = \frac{(\Sigma_0 g) y}{\gamma^2 + \langle g, \Sigma_0g \rangle}$
    - covariance: $\Sigma = \Sigma_0 - \frac{(\Sigma_0 g)(\Sigma_0 g)^*}{\gamma^2 + \langle g, \Sigma_0g \rangle}$.

    This is (according to Stuart) in the case of $q = 1$ ($\to y \in \R$) and $n \in \NN$ ($\to g,u \in \R^n$).

    I chose to work with $n=1$, since then it's easy to visualize the posterior and the chain.

    However, when $q=n=1$, I'm doubtful whether the expressions given for the posterior are actually
    correct, since when Stuart discusses the zero-noise limit in the case of $n=q$, he proves that
    then the posterior is independent of the prior (which for $q=n$ in this example is not the case).

    (Theorem 2.3 pg 463 [[cite:stuart_inverse_2010]])

*** Wasserstein convergence

    As noted above, we have an analytical expression for the posterior. The steady state of the
    Markov Chain will sample from this distribution, so we expect the distributions sampled
    from the steady state to converge to the posterior 
    when the sample length is increased (citation needed).

    #+CAPTION: Wasserstein distance between the sample distributions and the analyical posterior. The same chain as above is used.
    #+NAME: fig:stuart_21_convergence
    [[./figures/stuart_wasserstein_convergence_chain_report.png]]

** Bayesian inverse problem for $\G{u} = g (u + \beta u^3)$
   Since the observation operator is not linear anymore, the resulting posterior is not
   Gaussian in general. However, since the dimension of the input $u$ is 1, it can
   still be plotted.

   The concrete realization with:
   - $g = [3, 1]$
   - $u = 0.5$
   - $\beta = 0$
   - $y= [1.672, 0.91]$
   - $\gamma = 0.5$
   - $\eta \sim \N{0}{\gamma^2 I}$
   - prior $\N{0}{\Sigma_0=1}$
   however leads to a Gaussian thanks to $\beta = 0$. The mean is
   $\mu = \frac{\langle g,y \rangle}{\gamma^2 + |g|^2} \approx 0.58$. Plot: [[fig:stuart_22_density]]

   The pCN-Sampler with $\beta = 0.25$ (different beta) had an acceptance rate of 0.576.

   #+CAPTION: $N=5000, \mu \approx 0.58$
   #+NAME: fig:stuart_22_density
   [[./figures/stuart_example_22_q=2_N=5000.png]]

   For $\beta \neq 0$, the resulting posterior is not a Gaussian. Still $n=1$, so it can be
   plotted. Just numerically normalize the analytical expression of the posterior?

** Lorenz96 model
   #+INCLUDE: lorenz.org
** Perturbed Riemann problem for Burgers' equation
   #+INCLUDE: burgers.org
