* Results
** Analytic sampling from a bimodal Gaussian
*** Setup

    Attempting to recreate the "Computational Illustration" from [[cite:cotter_mcmc_2013]]. They use,
    among other algorithms, pCN to sample from a 1-D bimodal Gaussian
    $$\rho \propto (\N{3}{1} + \N{-3}{1}) \mathds{1}_{[-10,10]}.$$
    Since the density estimation framework for a known distribution is not quite clear to me from
    the paper, I don't expect to perfectly replicate their results.

    They use a formulation of the prior based on the Karhunen-Loéve Expansion that doesn't make
    sense to me in the 1-D setting (how do I sum infinite eigenfunctions of a scalar?).

    The potential for density estimation described in section is also not clear to me (maybe for
    a similar reason? What is $u$ in the density estimate case?).

    I ended up using a normal $\N{0}{1}$ as a prior and the potential described [[Potential for Bayes'-MCMC when sampling from analytic distributions][before]], and
    compared the following samplers:
    - (1) [[file:code.org::StandardRWProposer][~StandardRWProposer~]] ($\delta=0.25$) + [[file:code.org::AnalyticAccepter][~AnalyticAccepter~]]
    - (2) [[file:code.org::StandardRWProposer][~StandardRWProposer~]] ($\delta=0.25$) + [[file:code.org::StandardRWAccepter][~StandardRWAccepter~]]
    - (3) [[file:code.org::pCNProposer][~pCNProposer~]] ($\beta=0.25$) + [[file:code.org::pCNAccepter][~pCNAccepter~]]

    The code is in [[file:scripts/analytic.py][~analytic.py~]].

*** Result

    All three samplers are able to reproduce the target density [[fig:hist_bimodal]]

    #+CAPTION: Burn-in: 1000, sample-interval: 200, samples: 500
    #+NAME: fig:hist_bimodal
    [[./figures/bimodal_density_combined.png]]

    The autocorrelation decays for all samplers: [[fig:ac_bimodal]]. However, the pCN doens't
    do nearly as well as expected. This could be the consequence of the awkward
    formulation of the potential or a bad prior.

    #+CAPTION: AC of bimodal distribution. pCN takes forever to decorrelate
    #+NAME: fig:ac_bimodal
    [[./figures/analytic_standard_rw_pCN_50_5000.png]]


** Bayesian inverse problem for $\G{u} = \langle g,u \rangle$
   For $\G{u} = \langle g,u \rangle$ the resulting posterior under a Gaussian prior
   is again a Gaussian. The model equation is
   $$y = \G{u} + \eta$$
   with:
   - $y \in \R$
   - $u \in \R^n$
   - $\eta \sim \N{0}{\gamma^2}$ for $\gamma \in \R$

   A concrete realization with scalar $u$:
   - $u = 2$
   - $g = 3$
   - $\gamma = 0.5$
   - $y=6.172$
   - prior $\N{0}{\Sigma_0=1}$
   leads to a posterior with mean
   $\mu = \frac{(\Sigma_0g)y}{\gamma^2 + \langle g, \Sigma_0g \rangle} \approx 2$,
   which is what we see when we plot the result [[fig:stuart_21_density]].
   The pCN-Sampler with $\beta = 0.25$ had an acceptance rate of 0.567.
    
   #+CAPTION: $N=5000, \mu \approx 2$
   #+NAME: fig:stuart_21_density
   [[./figures/stuart_example_21_n=1_N=5000.png]]

   For $n>2$, the resulting posterior can not be plotted anymore. However, it is still Gaussian
   with given mean & covariance. Can just compare the analytical values to the sample values.
   Verify that the error decays like $\frac{1}{\sqrt{N}}$.
** Bayesian inverse problem for $\G{u} = g (u + \beta u^3)$
   Since the observation operator is not linear anymore, the resulting posterior is not
   Gaussian in general. However, since the dimension of the input $u$ is 1, it can
   still be plotted.

   The concrete realization with:
   - $g = [3, 1]$
   - $u = 0.5$
   - $\beta = 0$
   - $y= [1.672, 0.91]$
   - $\gamma = 0.5$
   - $\eta \sim \N{0}{\gamma^2 I}$
   - prior $\N{0}{\Sigma_0=1}$
   however leads to a Gaussian thanks to $\beta = 0$. The mean is
   $\mu = \frac{\langle g,y \rangle}{\gamma^2 + |g|^2} \approx 0.58$. Plot: [[fig:stuart_22_density]]

   The pCN-Sampler with $\beta = 0.25$ (different beta) had an acceptance rate of 0.576.

   #+CAPTION: $N=5000, \mu \approx 0.58$
   #+NAME: fig:stuart_22_density
   [[./figures/stuart_example_22_q=2_N=5000.png]]

   For $\beta \neq 0$, the resulting posterior is not a Gaussian. Still $n=1$, so it can be
   plotted. Just numerically normalize the analytical expression of the posterior?
** Geophysics example: Lorenz-96 model

*** Based on:

    Lorenz, E. N. (1996). Predictability—A problem partly solved. In Reprinted in T. N. Palmer & R. Hagedorn (Eds.), Proceedings Seminar on
    Predictability, Predictability of Weather and Climate, Cambridge UP (2006) (Vol. 1, pp. 1–18). Reading, Berkshire, UK: ECMWF.

*** Equation

    A system of ODEs, representing the coupling between slow variables $X$ and fast, subgrid
    variables $Y$.

    #+NAME: eqn:lorenz_X
    \begin{equation}
      \dv{X_k}{t} =                 - X_{k-1}(X_{k-2} - X_{k+1}) - X_k + F - hc \bar{Y}_k
    \end{equation}

    #+NAME: eqn:lorenz_Y
    \begin{equation}
      \frac{1}{c} \dv{Y_{j,k}}{t} = -bY_{j+1,k}(Y_{j+2,k} - Y_{j-1, k}) - Y_{j,k} + \frac{h}{J}X_k
    \end{equation}

    - $X = [X_0, ..., X_{K-1}] \in \R^K$
    - $Y = [Y_{j, 0} | ... | Y_{j, K-1}] \in \R^{J \cross K}$ \\
      $Y_{j,k} = [Y_{0,k}, ..., Y_{J-1,k}] \in  \R^J$
    - $\bar{Y}_k = \frac{1}{J}\sum_j Y_{j,k}$
    - periodic: $X_K = X_0$, $Y_{J,k} = Y_{0,k}$
    - Parameters $\Theta = [F, h, c, b]$
    - $h$: coupling strength
    - $c$: relative damping
    - $F$: external forcing of the slow variables (large scale forcing)
    - $b$: scale of non-linear interaction of fast variables
    - $t = 1 \Leftrightarrow 1$ day (simulation duration is given in days)

**** $b$ or $J$?
     In the original paper, the equations are given in a different form, namely all
     explicit occurences of $J$ above (in the fast-slow interaction) are replaced by
     $b$. Since in both concrete realizations (1996 & 2017) are identical and conviniently
     have $b=J=10$, the difference doesn't lead to different results for that setup.

**** Where is $F$?
     In the original paper (at least the one I found), the forcing term $F$ for the
     slow variables is missing in the equation, however it is referenced in the text,
     as well as present in a simpler (only slow variables) setup.

     Maybe I have a shitty version of the paper?

**** "Looking ahead" vs. "Looking back"
     Comparing nonlinearity terms
     $$-X_{k-1}(X_{k-2} - X_{k+1})$$
     $$-bY_{j+1,k}(Y_{j+2,k} - Y_{j-1, k})$$
     for a given $Y_{k}$, does the "direction" of the $Z_{k\pm 1}Z_{k\pm 2}$ ($Z=X,Y$) matter?

     I don't think so, since the interaction with the other variable is only via point-value
     and average, and the nonlinearity is periodic.
     
     A bit more formally:
     The PDE is invariant under "reversing" of the numbering:
     $Y_{j,k} \rightarrow Y_{J-j,k}$ which is the same as switching $+ \leftrightarrow -$ in
     the only "asymmetric" term.

     Addendum 2 days later: Need to define more clearly what it means for the direction to
     _matter_. In the original paper on page 12, it is described how "active areas [..] propagate
     slowly eastward", while "convective activity tends to propagate westward within the active
     areas" (rephrased from paper). The paper also explicitly mentions the signs of the subscripts
     in that context. So some characteristics of the solution are definitely affected.
     What about the stuff we care about (statistical properties, chaotic behaviour)?

**** Initial conditions for the MCMC observation operator
     In [[cite:schneider_earth_2017]], they describe their algorithm as:

     "The objective function for each sample is accumulated over T = 100 days, using the end state
     of the previous forward integration as initial condition for the next one, without discarding
     any spin-up after a parameter update."

     (Disregarding the fact that for this chaotic ODE and such a long simulation time the initial
     conditions are virtually irrelevant)
     
     From a theoretical stand-point, should the observation operator $\G{u}$ not depend _only_ on
     $u$, the parameters we try to estimate in the algorithm? And should it not be deterministic,
     all the (stochastic) observational noise encoded in the error term $\eta$?

     Does this changing of the IC every iteration not pose a problem for the convergence of the
     algorithm?\\

     Also does the spin-up they mention refer to discarding the first part of the simulation
     when computing steady-state averages? It somehow doesn't make sense to talk about a
     Markov-spin-up in the context of a single ODE-simulation, but ALSO it doens't make sense
     to talk about averages since from my understanding the MCMC-algorithm doesn't have to
     compute averages, it just looks at the final state?

     
*** Properties

    - For $K=36$, $J=10$ and $\Theta = [F, h, c, b] = [10, 1, 10, 10]$ there is chaotic behaviour.

    - The nonlinearities conserve the energies within a subsystem:
      - $E_X = \sum_k X_k^2$

        $\frac{1}{2} \dv{(\sum_k X_k^2)}{t} =
         \sum_k X_k \dv{X_k}{t} =
         -\sum_k (X_k X_{k-1} X_{k-2} - X_{k-1} X_{k} X_{k+1}) =
         0$,

        where the last equality follows from telescoping + periodicity
      - $E_{Y_k} = \sum_j Y_{j,k}^2$

        which follows analogously to the $X$ -case

    - The interaction conserves the total energy:
      - $E_{T} = \sum_k (X_k^2 + \sum_j Y_{j,k}^2)$

        $\frac{1}{2} \dv{E_{T}}{t} =
         \sum_k X_k \dv{X_k}{t} + \sum_j Y_{j,k} \dv{Y_j,k}{t} =
         \sum_k X_k (- \frac{hc}{J} \sum_j Y_{j,k}) + \sum_j Y_{j,k} (\frac{hc}{J} X_k) =
         \sum_k - \frac{hc}{J} X_k (\sum_j Y_{j,k} + \frac{hc}{J} X_k (\sum_j Y_{j,k})) = 
         0$

    - In the statistical steady state, the external forcing $F$ (as long as its positive) balances
      the dampling of the linear terms.

    - Averaged quantities
      - $$\expval{\phi} = \frac{1}{T} \int_{t_0}^{t_0 + T} \phi(t) \dd{t}$$ (or a sum over discrete values)
      - Long-term time-mean in the statistical steady state: $\expval{\cdot}_{\infty}$
      -
        #+NAME: eqn:equilibrium_X
        \begin{equation}
          \expval{X_k^2}_\infty = F \expval{X_k}_{\infty} - hc \expval{X_k\bar{Y_k}}_\infty \forall k
        \end{equation}
        (multiply $X$ -equation by $X$, all $X_k$ s are statistically equivalent, $\dv{\expval{X}}{t} = 0$ in steady state)
      -
        #+NAME: eqn:equilibrium_Y
        \begin{equation}
          \expval{\bar{Y_k^2}}_{\infty} = \frac{h}{J} \expval{X_k \bar{Y_k}}_{\infty} \forall k
        \end{equation}

*** Model implementation
    Implementing the model in python and using a locally 5-th order RK solver yields the following
    results (inital conditions are just uniformly random numbers in $[0,1)$ since they don't matter
    for the long-term evolution of the chaotic system):

**** Reproducting the results of the original paper
     Running the setup with $K=36, J=10, (F, h, c, b) = (10, 1, 10, 10)$ given the following states
     [[fig:state_lorenz_30]], [[fig:state_lorenz_60]],
     which qualitatively agree with the results from Lorenz.

    #+CAPTION: System after 30 days
    #+NAME: fig:state_lorenz_30
    [[./figures/lorenz96_middle.png]]

    #+CAPTION: System after 60 days
    #+NAME: fig:state_lorenz_60
    [[./figures/lorenz96_last.png]]

    The decay of the linear term and the forcing of the slow variables balance out after reaching the
    steady state, however there is a much bigger fluctuation in the energy than expected [[fig:lorenz_energy]].

    #+CAPTION: Energies in the system. $E_X >> E_{Y_k} > 0$
    #+NAME: fig:lorenz_energy
    [[./figures/lorenz96_energies.png]]

**** Equilibrium averages
     Analysis suggests certain long-term averages to be equal in the equilibrium.

    #+CAPTION: RMSE for long-term averages [[eqn:equilibrium_X]] and [[eqn:equilibrium_Y]]. Averaged over 10 runs
    #+NAME: fig:lorenz_rmse
    [[./figures/equilibrium_error_n=10.png]]
    

------------------- added Fri 22.05 -----------------------
*** MCMC
**** Setup
     Denote the Lorenz-96 system [[eqn:lorenz_X]], [[eqn:lorenz_Y]] with parameters $u = [F, h, c, b]$ as
     $\mathcal{M}[u]$. It acts on the initial condition $z_0 = [X_0, Y_0] \in \R^{K(J+1)}$ to evolve
     the system for $N_t$ timesteps to generate the ? (find a good name for the "history"  of the
     system)
     $Z = \left[ \smash{{}^{X_1}_{Y_1}} | \cdots | \smash{{}^{X_{N_t}}_{Y_{N_t}}}  \right] \in \R^{K(J+1) \cross N_t}$:
     $$Z = \mathcal{M}[u] z_0$$

     The "moment function" $f(z): \R^{K(J+1)} \to \R^{5K}$ (why can't we just use the state of
     the system directly?):

     \begin{align}
       f(z) &=
       \begin{bmatrix}
         X \\
         \bar{Y} \\
         X^2 \\
         X \bar{Y} \\
         \overline{Y^2}
       \end{bmatrix}
     \end{align}

     Defining a potential energy for a simulation with duration $T$

     #+LABEL: eqn_lorenz_J
     \begin{equation}
       J(Z) = \frac{1}{2} \norm{\expval{f(z)}_T - \expval{f(\tilde{z})}_\infty}_\Sigma,
     \end{equation}
     where $\tilde{z}$ is the output of a simulation with the true parameters $\tilde{u} = \Theta$ and
     $\expval{\cdot}_\infty$ is approximated by an average over time $\tilde{T} >> T$. $\Sigma$ is
     the covariance matrix of the noise.

     Putting it all together yields the following equation for the MCMC:

     $$y = J(\mathcal{M}[u] z_0) + \eta$$
     with:
     - $y \in \R^{5K}$: _moment function of last state or mean of moment function_?
     - $u \in \R^4$: Parameter vector
     - $J(\mathcal{M}[\cdot] z_0) : \R^4 \to \R^?$: Observational operator
     - $\eta \sim \N{0}{\Sigma}$, where $\Sigma = r^2 [\text{var}(f(\tilde{z}))]$,
       where $r \in \R$ is the "noise level"

**** What is actually going on?
     They describe the noise with a covariance matrix $\Sigma \in \R^{5K \cross 5K}$, implying
     that $\G{u} \in \R^{5K}$. However, in the description of the MCMC-algorithm they talk about
     computing the objective function, which I understand as meaning that $\G{u} \in \R$?
     Also the scariest thing is that python doesn't complain either way, there is some shady
     broadcasting going on in numpy.

**** Independently of the correctness of the simulation, there is a serious feasibility challenge
     As it stands now, doing a small run ($K=6, J=4, T=10$) takes 14 seconds. This means a MCMC
     sampling takes approximately \infty.

     The most time ($1/3$ of the total time) is spend calculating the nonlinear terms. They already
     use [[https://numpy.org/doc/1.18/reference/generated/numpy.roll.html?highlight=roll#numpy.roll][~np.roll~]] and no explicit loops, so I'm not confident that they can be sped up significantly.
     Maybe trying out something like http://numba.pydata.org/ could give some speedup, I have no
     firsthand experience however.

     Since the sampling is (approximately) embarassingly parallel, I think I could get a speedup
     of $\mathcal{O}(4)$ on my laptop.
